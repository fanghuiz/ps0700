[{"authors":null,"categories":null,"content":"To be updated\n Setting up R and RStudio Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","date":1548738000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1549515600,"objectID":"389d456065c47b7be0bc1c147dc1b25d","permalink":"https://fanghuiz.github.io/ps0700/tutorial_r/","publishdate":"2019-01-29T00:00:00-05:00","relpermalink":"/ps0700/tutorial_r/","section":"tutorial_r","summary":"To be updated\n Setting up R and RStudio Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"To be updated\n Setting up Stata Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","date":1548738000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1549515600,"objectID":"3e030c29ad53ebb0c0407986b4254cb1","permalink":"https://fanghuiz.github.io/ps0700/tutorial_stata/","publishdate":"2019-01-29T00:00:00-05:00","relpermalink":"/ps0700/tutorial_stata/","section":"tutorial_stata","summary":"To be updated\n Setting up Stata Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Table of Contents    In the class we talked about surveys having high external validity, but weak in internal validity. Does external validity take precedence (over internal validity) in terms of importance, or vice versa? Is random sampling and randomization the same thing? How can we account for coverage error in experimental studies?    In the class we talked about surveys having high external validity, but weak in internal validity. Does external validity take precedence (over internal validity) in terms of importance, or vice versa? I would say that in general, it is more important to establish internal validity than external validity. If we can ensure internal validity, at the very least, we can claim to have gained some localized knowledge ($X$ causes $Y$ in the sample we have studied), even if this knowledge might not hold in another context.\nHowever, if we cannot be sure that the findings in our current study is internally valid (i.e. if we are unable to establish a credible claim that it is indeed $X$ that caused a change in $Y$, rather than other confounding factors), then what’s the point of generalizing this invalid claim? Only when we have confidence in the internal validity of a study (more localized knowledge), then having external validity will be useful (allow us to expand on this knowledge). Otherwise, generalizing a wrong-headed conclusion only compounds the initial error, like adding more heights to a building with a faulty foundation.\n  Is random sampling and randomization the same thing? Random sample refers to a sample (i.e. the subset of population that we include in the study) where each unit is chosen randomly. This concerns the cases or subjects in the study.\nRandomization (a.k.a random assignment) refers the process of randomly assigning each unit in our study to receive the treatment or not. This concerns whether the units/subjects (who are already included in the study) is receiving the treatment, or will they in the control group.\nHow can we account for coverage error in experimental studies? Depending on how the subjects are recruited into the experiment, coverage errors in experimental studies can be difficult to avoid. Recall that many experiments, especially lab experiments, rely on convenience sample, which usually leads to part of the population not being covered in the sampling process. If subjects are recruited among the college undergraduates, then anyone who is not a undergraduate from that university is excluded from the sample.\nThis problem can be difficult to “account for” if we are using convenience sample, since it is built-in to the sampling process. However, other types of non-laboratory based experiments (e.g. survey experiments or field experiments) often have better coverage, which mitigates (though does not 100% eliminate) the problems of non-representative sample that comes with coverage errors.\n","date":1551416400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552881600,"objectID":"03378b75cd57c065c9af896cb2ac9310","permalink":"https://fanghuiz.github.io/ps0700/post/2019-03-01-survey/","publishdate":"2019-03-01T00:00:00-05:00","relpermalink":"/ps0700/post/2019-03-01-survey/","section":"post","summary":"Q\u0026A on external validity of survey research, random sample vs randomization, and coverage errors in samples used for experimental studies","tags":["Survey","Sampling","Polling"],"title":"Q\u0026A Week 8: Sampling and Survey Research","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    Can you explain more about the connection between Mill’s method of difference and experiments? Is the concern for external validity problems only apply to method of difference, or method of agreement as well? Is selecting on dependent variable only a problem for method of agreement? The lecture mentioned that method of difference has trouble estimating “multiple causes”. What are some examples of “multiple causes” cases?    Can you explain more about the connection between Mill’s method of difference and experiments? The two have very similar causal logic. They both try to establish a causal claim (difference in $Y$ can be attributed to changes in $X$) by leveraging on the fact that the treatment group (X = 1) and the control group (X = 0) are similar on other confounding variables ($Z$), except the treatment variable ($Y$) — since the two groups are similar in other aspects except with regard to $X$, any observed difference in $Y$ must be caused by the difference in $X$.\nMethod of difference try to approximate a comparable treatment and control group by selecting cases with similar attributes except $X$ (mostly based on theory and domain knowledge about what factors could be potentially confounding variables). Experiments try to achieve this by randomly assigning the treatment.\nIs the concern for external validity problems only apply to method of difference, or method of agreement as well? It is a problem in both types of designs. External validity issue is present in all studies where we only have a small number of non-randomly selected cases.\nIs selecting on dependent variable only a problem for method of agreement? Yes it is only a problem for comparative designs that select cases using methods of agreement.\nWe say a study is “selecting on dependent variable” when the decision criterion to include certain units into (or exclude from) the study sample is correlated with the value of the dependent variable.\nFor method of agreement, we are comparing cases with the same outcome but differs in the value of independent variable. In another word, the reason we are including these cases in the comparison is because that they share the same outcome, and other cases are excluded because they have a different outcome — the decision criterion for sample selection is directly related to the status of dependent variable.\nDesigns using methods of difference for case selection are not selecting on dependent variable. In this method, the criterion to select cases to be included in the sample is not related to what the outcomes are. Instead, we are selecting cases based on the independent variables — we are comparing the cases that are similar in all the independent variables, except one crucial explanatory factor that we are interested in.\nThe lecture mentioned that method of difference has trouble estimating “multiple causes”. What are some examples of “multiple causes” cases? An event or outcome has multiple causes when there are more than one factors that could have lead to the outcome. For example, why the U.S has low voter turnout? There could be multiple factors for this: no compulsory voting; low interests in politics; election day is not a national holiday; two party system; winner-take-all system etc.\nMost of the phenomenon we are interested are quite complex, so we should expect there to be multiple-causes most of the time.\n","date":1550811600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552881600,"objectID":"74dc27c1cddface553fb66aae7b0e9f9","permalink":"https://fanghuiz.github.io/ps0700/post/2019-02-22-comparative/","publishdate":"2019-02-22T00:00:00-05:00","relpermalink":"/ps0700/post/2019-02-22-comparative/","section":"post","summary":"Q\u0026A on the logics of Mill's method of comparison, externality validity issues in small-N comparative designs, and selecting on dependent variable","tags":["Comparative studies"],"title":"Q\u0026A Week 7: Comparative Studies","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    Costs of Voting  Can you explain a bit more about the table for costs of voting?  Game Theory and Government Shutdown  Trump was prolonging the shutdown in order to get funding for the wall, is that a game theory/strategic interaction scenario?     Costs of Voting Can you explain a bit more about the table for costs of voting? This is the table I have in the recitation slides:     Voted = Yes Voted = No     Election outcome = Preferred candidate won Benefits - Costs of Voting Benefits   Election outcome = Preferred candidate lost - Costs of Voting Zero    First, we have a few assumptions when analyzing the decision to vote from a rational choice framework:\n Cost of voting is negative if we vote; and is zero if we do not voting Benefit is positive if our preferred candidate wins; and is zero if our preferred candidate loses Chance of any individual vote changing the outcome is very low (close to zero)  The intuition behind the table is that no matter what is the election outcome (preferred candidate win or lose), for us personally, the net benefit is always higher if we do not vote, than if we vote.\n If our preferred candidate wins (first row), Benefits \u0026gt; Benefits - Costs of Voting. Net benefit is higher if we do not vote. If our preferred candidate loses (second row), Zero \u0026gt; - Costs of Voting. Net benefit is higher if we do not vote.  Game Theory and Government Shutdown Trump was prolonging the shutdown in order to get funding for the wall, is that a game theory/strategic interaction scenario? Yes! Threatening or prolonging government shutdown in order to leverage a “better deal”, when viewed from a strategic interaction lens, is quite similar to the game of chicken (a sort of brinksmanship).\nThis is a situation where both players will benefit if both sides yield (take a compromise budget deal), both players will lose if neither side yield (government shutdown), but if only one player yields and the other doesn’t (Trump gives up, Congressional Democrats do not), then the player that yields loses and the other player benefits (no funding for wall, government shutdown ends).\nSee this NPR article for more in depth discussion on the incentives both sides faced that shaped this negotiation into a political brinksmanship, and this FiveThirtyEight article on why we (the voters) are partly to blame for this.\n","date":1550206800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551848400,"objectID":"76bbfa99208e3f7cddbfd687f4d23219","permalink":"https://fanghuiz.github.io/ps0700/post/2019-02-15-rational-choice/","publishdate":"2019-02-15T00:00:00-05:00","relpermalink":"/ps0700/post/2019-02-15-rational-choice/","section":"post","summary":"Q\u0026A on costs of voting, and government shutdown through game theory lens","tags":["Formal model","Game theory","Rational choice"],"title":"Q\u0026A Week 6: Formal Models and Game Theory","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    Natural Experiments  In class you mentioned “Natural experiments based on geographical boundaries can be complicated by human factors”. Can you explain a bit more what this means? How would we know if the “as-if randomization” assumption is valid?  Observational studies  Is there any way to get rid of confounding variables in observational studies? How are longitudinal studies and cross-sectional studies different?     Natural Experiments In class you mentioned “Natural experiments based on geographical boundaries can be complicated by human factors”. Can you explain a bit more what this means? Recall that the key assumption in a natural experiment design that ensures internal validity is that the treatment assignment is random or “as-if” random. In another word, we have to ask, is the treatment assignment correlated with any other factors that could potentially cause the observed difference between treatment and control group? If yes, then the assumption does not hold and the study’s internal validity is weakened. If no, then the assumption of “as-if” randomization holds.\nIn the study on whether money from lottery will increase happiness, the assumption is that the treatment (winning money from lottery) is randomly assigned among lottery buyers, hence whether someone is in the treatment group (lottery winners) or the control group (lottery losers) is not correlated with other factors that affects their happiness. In another word, treatment assignment (whether someone gets money) is independent of other confounding factors that could have affected the outcome (happiness).\nIn studies that leverage on geographical boundaries for natural experiment opportunities, the generic set-up is to compare Area A (treatment group) on one side of the geographical boundary that have received the treatment, with Area B (control group) on the other side of the boundary that have not received the treatment. This means that we have to ask, is the treatment assignment (being on one side of the boundary vs the other side) correlated with any other factors that could explain the difference in outcomes between Area A and Area B?\nSo what I meant by “natural experiments based on geographical boundaries can complicated by human factors“ was that, sometimes how the geographical boundaries are drawn, is not independent of the characteristics of the humans/political actors that draw these boundaries (i.e. the division introduced by the boundary is not random). If the reasons for how boundaries are drawn correlates with reasons that could explain the outcome, then the “as-if” randomization assumption would not hold.\nThink about Posner (2004) we read for class, where Posner found that the relative size of the two ethnic groups (treatment) within each country explained why the cultural differences between the Chewa and Tumbuka ethnic groups are politically salient in Malawi but not in Zambia (outcome). He argued that the treatment assignment (being in a country where the two ethnic groups is relatively large vs relatively small) is “as-if“ random (assignment is uncorrelated with other factors that could explain the outcome), because “like many African borders, the one that separates Zambia and Malawi was drawn purely for [colonial] administrative purposes, with no attention to the distribution of groups on the ground” (Posner 2004: 530).\nIf however, the boundary that separates Zambia and Malawi are drawn for reasons that potentially correlate with factors affecting inter-group interaction (say for example, natural resource availability), then the treatment assignment is no long “as-if” random.\nHow would we know if the “as-if randomization” assumption is valid? Since we have no control over the treatment assignment process in natural experiments, we cannot really “prove” whether this “as-if” randomization assumption is valid. All we can do is provide evidence to show that this assumption is plausible.\nFor example, we can rely on theory and background knowledge to make the case: assignment through lottery is plausibly random because we know how the winner are chose.\nAnd for the Posner (2004) study, if there were some qualitative evidence (e.g. written records of how boundaries were decided) showing that the boundary was indeed “drawn purely for [colonial] administrative purposes, with no attention to the distribution of groups on the ground”, then that would be an important piece of evidence to support the “as-if” randomization claim.\nWe can also provide empirical evidence. Recall that randomly assignment treatment will give us comparable treatment and control groups, i.e. the groups on average, would be similar to each other in terms of any potential confounding variables. So we should expect that “as-if” randomization process should give us such comparable groups as well.\nResearchers can measure the potential confounding variables and empirically test if the treatment and control groups are similar in those aspects. If we do not find any significant difference between the two groups in terms of those potential confounders, then that would be a piece of evidence supporting the “as-if” randomization assumption.\nObservational studies Is there any way to get rid of confounding variables in observational studies? Confounding variables will always be present (we cannot \u0026ldquo;get rid of them\u0026rdquo; per se), but we can reduce the bias to our inference/conclusion introduced by any confounding variables.\nWhenever we want to investigate if $X \\rightarrow Y$, there will be confounding variables $Z$ lurking behind the scenes, that’s just the feature of the world we live in. These confounding variables will introduce bias to our inference, if we mistakenly conclude that the change in $Y$ is caused by $X$, while in fact the change in $Y$ was caused by $X$ and $Z$ (or $Z$ alone). This bias is often known by the jargon omitted variable bias.\nWhen designing a study to investigate if $X \\rightarrow Y$, one of our goals is to reduce any potential bias introduced by confounding variables, in order to isolate the effects of $X$ on $Y$ (how much of the change in $Y$ can be attributed to $X$, instead of $Z$).\nTwo common ways to reduce this bias in observational studies:\n Statistically adjusting/controlling for observable confounding variables (i.e. include the “omitted” confounding variables in the statistical model, at least for those we have the data for). If our data has multiple time points (i.e. panel data or time series data), statistically adjusting/controlling for observable and unobservable confounding variables by leveraging on the temporal nature of the data.  The jargon for these different techniques to isolate the effects of $X$ on $Y$ is “identification strategy” — strategies that help us to identify the effects of $X$ on $Y$. Randomized experiment, natural experiments, statistically adjusting for confounders are different types of identification strategies we can use.\nHow are longitudinal studies and cross-sectional studies different? We have a longitudinal study if we have data for each unit at multiple time points, i.e. every unit is measured more than once. For example, a study on the effects of emergency events boosting presidential approval ratings (i.e. rally-the-flag effects) would be a longitudinal study (or more specifically, time series) — the unit of analysis is presidential approval ratings, and we have measures for this unit at multiple time points, before and after the emergency events.\nA cross-sectional study is one where we only have data for each unit at one time points. If we were to examine whether partisanship affects how individuals evaluate the president’s response to a emergency event, say a devastating hurricane, using a survey conducted after the hurricane, then that would be a cross-sectional study — the unit of analysis is individual survey respondents, and we only have measures for the same person at one point in time (the time they responded to the survey).\n","date":1548997200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551848400,"objectID":"d612a07fee1f3f579c8b7ba7b1c7551d","permalink":"https://fanghuiz.github.io/ps0700/post/2019-02-01-natural-experiment/","publishdate":"2019-02-01T00:00:00-05:00","relpermalink":"/ps0700/post/2019-02-01-natural-experiment/","section":"post","summary":"Q\u0026A on natural experiments leveraging on geographical boundary, the 'as-if' randomization assumption, and dealing with confounding variables in observational studies","tags":["Experiments","Natural experiment"],"title":"Q\u0026A Week 4: Natural Experiments and Observational Studies","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    About Informed Consent    Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment? Does knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome?   About the Montana GOTV Experiment    The Montana experiment misled the people by using official seal. How did they get the project approved in the first place? How did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome.   About Experiments on Development Programs    Are there examples of ethical and effective anti-poverty experiments? Not a question, just an interesting observation \u0026ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate.      About Informed Consent Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment? Yes, informed consent is an essential element of research ethics.\nGenerally speaking, we have to inform the participants the purpose of our research (e.g. “This is a study about attitudes towards political candidates), though we do not have to tell them the exact hypothesis of the study.\nIt is also important that the informed consent form has to let the participants know if there is any potential benefits or harms by taking part in the study, any compensations or incentives, confidentiality or privacy of the data, their rights to decline and to withdraw, so they can make an informed decision about participating. In most cases, political science experiments only involve \u0026ldquo;minimal risks\u0026rdquo;, i.e. about the same probability and magnitude of harm we would experience in daily life.\nFor more details on the important elements to include when obtaining informed consent, see this guide from Pitt IRB, or American Psychological Association (APA) ethics code (Section 8.02). It is possible to request waivers with adequate justification (see here for an overview of the requirements).\nDoes knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome? Quite likely! One possibility is Hawthorne effect: simply being part of the experiment and knowing that you are being observed might change your behavior or how you respond, compare to everyday life scenario.\nA more general phenomenon (some argue subsumes the Hawthorne effect) is called demand characteristics (also see textbook p.178), referring to how participants\u0026rsquo; interpretation of the experiment\u0026rsquo;s purpose could potentially change their behaviors (e.g. behave in ways conforming to what they think the researchers want to observe, or they might behave in ways contradicting to what they perceived as the researchers\u0026rsquo; hypothesis).\nIt is worth noting however, that not all experiments are equally affected by this potential problem. We might expect that experiments looking at behaviors that are more susceptible to social desirability bias are more vulnerable to bias introduced by demand characteristics, than those looking at more benign phenomenons.\nWhile it is difficult to eliminate this effect completely in most experiments, some strategies exist. For example, researchers can devise a design that uses covert or unobtrusive treatments, so the participants are unaware that they are part of an experiment (e.g. Enos 2014, Sands 2017).\nDeception is another common, though deeply controversial strategy. For example audit experiments often rely on deception to examine socially undesirable behaviors such as discrimination (e.g. Butler and Broockman 2011), norms or rules violation (e.g. Findley, Neilson and Sharman 2014).\nOf course, the use of deception always has to be justified in the ethics review process. See this newsletter (p.13-19) for further discussion on the ethics of using deception in field experiment involving public officials as subjects.\nAbout the Montana GOTV Experiment The Montana experiment misled the people by using official seal. How did they get the project approved in the first place? Only the people involved in the process would ever know! If I were to hazard a guess (take it with many many grains of salt), it is possible that the review process did not see the mailer as being intentionally misleading. Among the commotion in the follow-up to this controversy, one detail about the mailer did not get much attention — there was in fact a disclaimer line disclosing that the mailer is part of a academic study (below the boxes indicating candidate ideology).\n  Mailer from the experiment. Squint a little to see the disclaimer. Retrieved from Internet Archive    Maybe it’s too much of a fine print, but it’s there. So you could make the argument that they are not actively trying to deceive the recipient about who is sending the mailer out, and this might be part of reason why the proposal was approved. Again, I have to emphasize that this is all speculations on my part.\nHow did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome. We might never know! After the whole debacle, the study is unpublishable. Partly due to the ethical issue, partly because the data is likely unusable, given the spillover/contamination effect caused by the news coverage. After the news outlets reported about the story, those in the treatment group who have received the mailer would have known about where this mailer comes from and why they are receiving it (treatment is contaminated by extraneous factors that the researchers did not intend to provide), and those in the control group would also have known about the information in the mailer despite not receiving one (treatment spillover).\nAbout Experiments on Development Programs Are there examples of ethical and effective anti-poverty experiments? There are many examples of using randomized experiments to evaluate the impacts of anti-poverty programs. Some good places to look for them: Poverty Action Lab (J-PAL) (research center at MIT), GiveWell (nonprofit focused on effective charities).\nNot a question, just an interesting observation \u0026ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate. Hmm, this is really interesting to know. So if the experiment shows that UBI improves some aspects of life quality (e.g. household income, children\u0026rsquo;s education), but also has other \u0026ldquo;side-effects\u0026rdquo; such as increases divorce rate, from the policy-makers\u0026rsquo; position, what should they make of this? What kind of \u0026ldquo;side-effects\u0026rdquo;, or how much, would be considered as a \u0026ldquo;reasonable\u0026rdquo; level of trade-off? Back to what we discussed in the beginning of the course, empirical evidence does not always lead to a neat solution to normative questions.\n","date":1548392400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549429200,"objectID":"42af2b4d2044a2804ed2adc2e35dc482","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-25-experiment/","publishdate":"2019-01-25T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-25-experiment/","section":"post","summary":"Q\u0026A on informed consent in research, the controversial Montana GOTV experiment, and experiments on development programs","tags":["Experiments","Ethics"],"title":"Q\u0026A Week 3: Experiments and Ethics","type":"post"},{"authors":[],"categories":null,"content":" Example: Racial Resentment Scale Racial resentment scale is commonly used to measure symbolic racism. The scale contains four items, for each question, respondents indicate whether they agree or disagree with the statement on a five-point scale. The question wording and the respective variable number as appeared in American National Election Studies (ANES) 2016 are given below:\n V162211: \u0026lsquo;Irish, Italians, Jewish and many other minorities overcame prejudice and worked their way up. Blacks should do the same without any special favors.\u0026rsquo; V162212: \u0026lsquo;Generations of slavery and discrimination have created conditions that make it difficult for blacks to work their way out of the lower class.\u0026rsquo; V162213: \u0026lsquo;Over the past few years, blacks have gotten less than they deserve.\u0026rsquo; V162214: \u0026lsquo;It’s really a matter of some people not trying hard enough, if blacks would only try harder they could be just as well off as whites.\u0026rsquo;  The assumption is that agreeing with statement 1 and 4 (or disagreeing with statement 2 and 3) are indications of resentment towards African Americans.\nValidity Test Construct validity To test for construct validity, we need to demonstrate that the indicator predicts what it is supposed to predict.\nOne aspect of construct validity is convergent validity: if theoretically we expect X and Y to be positively related, do we see a positive correlation between the indicator for X and Y?\nIn this case, theoretically we might expect that feelings of resentment towards African Americans would correlate with negative affective attitudes towards the group.\nFor illustration purposes, let\u0026rsquo;s just use a single statement from the resentment scale, statement 2 (V162212) about the effects of slavery and see if people\u0026rsquo;s answer to this questions correlates with their feeling thermometer score towards Blacks (V162312).\n. twoway (scatter V162312 V162212) (lfit V162312 V162212)    We see that higher disagreement with the statement (1 = Strongly Agree, 5 = Strongly Disagree) correlates with lower scores on the feeling thermometer (higher value means warmer feeling towards the group, lower value means colder feeling). Resentment towards African Americans (as indicated by denying the effects of slavery on their current day hardship) indeed predicts a more negative attitudes towards them (as indicated by expressing less warm feelings).\nAnother way to demonstrate construct validity is to show divergent/discriminant validity: if theoretically we do not expect X and Y to be related, do we then see a low or weak correlation between them?\nFor instance, perhaps we do not expect feelings of racial resentment to be correlated with feelings towards the Supreme Court (V162102).\n. graph twoway (scatter V162102 V162212) (lfit V162102 V162212)    We see that there is no discernible correlation between responses to statement 2 and feelings towards the Supreme Court.\nReliability Test One common way to quantify the reliability of a multiple indicator scale is to calculate the Cronbach\u0026rsquo;s alpha $\\alpha$.\nThis can be done in Stata using a simple command alpha, followed by the list of variables used in the scale.\n. alpha V162211 V162212 V162213 V162214 Test scale = mean(unstandardized items) Reversed items: V162211 V162214 Average interitem covariance: 1.090995 Number of items in the scale: 4 Scale reliability coefficient: 0.8451  In the output, we can see a \u0026ldquo;Scale reliability coefficient\u0026rdquo;, which is ~0.8 in this case. A general rule of thumb is that \u0026gt; 0.8 indicates rather high reliability, and anything below 0.7 is a sign of unreliable scale. So in this case, the four-item Racial Resentment Scale has rather high reliability.\n","date":1547874000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"92651ff548424570f1150e6478abb4ac","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/","publishdate":"2019-01-19T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-19-example-measurement-stata/","section":"post","summary":"Simple demonstration of reliability and validity tests in Stata using racial resentment scale as an example","tags":["Measurement","Stata"],"title":"Example: Testing for measurement validity and reliability","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    Types of measurement Errors    How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement? Which error (systematic vs random) is worse? Which one should we try to avoid more? About the True Score Theory $T = X + \\epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$? If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?   Measurement Reliability and Validity    Is there a good analogy to help remembering the difference between validity and reliability? What are some examples of face validity? When do we test for construct vs face validity? About the article we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?   Levels of Measurement    Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures? Can a measure be both interval and ratio?      Types of measurement Errors How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement? Systematic errors affects all units in the sample in the same direction (all measured values are consistently more positive or more negative than true value). For example, self-reported measure of turnout is likely to have positive systematic error \u0026ndash; (most) people tend to over-report, saying that they have voted even though they have not.\nRandom errors do not affect all units in the sample in a consistent way \u0026ndash; some units will be more positive than true value, some units will be more negative than true value. Let\u0026rsquo;s use the self-reported turnout as an example again. Perhaps people\u0026rsquo;s transient feelings about the current election affect whether they are likely to say they have voted or not \u0026ndash; those who happened to read a positive news story about the election are more likely to over-report having voted, and others who happened to read a negative news story are more likely to under-report.\nVery often both types of errors could be present, so we need to think carefully about the sources of potential errors. For example, crime statistics can be very noisy, with a lot of random errors introduced at various stages of collecting such data. Furthermore, statistics on certain types of crimes might additionally have systematic errors: for example, domestic abuse might be systematically biased downwards if victims under-report due to fear of retaliation.\nWhich error (systematic vs random) is worse? Which one should we try to avoid more? Both types of errors are bad news! But they affect our analysis in different ways.\nHigh random errors will add more noise/variability to our data, which will make it harder to detect the presence of a significant correlation between X and Y. In another word, noisy measures are bad because it increases the likelihood of false negative \u0026ndash; we are likely to mistakenly infer there is no relationship between X and Y, when in fact there is.\nFor systematic errors, recall that indicators with high systematic errors are invalid, i.e. they are not capturing the concept of interest accurately. In such case, an invalid indicator will never lead us to the right conclusion (think of a road sign that points to the wrong direction), even if the indicator is measured with zero random error.\n  One of them has got to be invalid..   In terms of which type of error is worse, one way I think about this is that invalid indicator is more like a fatal disease, and unreliable indicator is more like a non-fatal but chronic disease that requires lots of care. So if a study is using invalid indicators, we cannot draw any meaningful inferences about the phenomenon we are investigating (the study is \u0026ldquo;dead\u0026rdquo;), while unreliable indicators make it harder to detect a true positive (increases uncertainty, but does not spell doom).\nThe textbook also has a good discussion on the different problems associated with measurement reliability and validity in political science (p.143-145).\n    Systematic error = High Systematic error = Low     Random error = High Very very bad \u0026bull; Invalid and unreliable measure \u0026bull; Lots of noise, and signal is pointing at the wrong direction Problematic, but can live with \u0026bull; Valid, but unreliable measure \u0026bull; Lots of noise, harder to detect the signal; More likely to get false negative   Random error = Low Problematic \u0026bull; Invalid, but reliable measure \u0026bull; Measure does not capture the concept of interests; Conclusion does not bear on the actual phenomenon of interest Awesome! \u0026bull; Valid and reliable measure \u0026bull; Move along    Ideally, we should try to minimize both types of measurement errors. Degree of random error can be empirically assessed (e.g. using Cronbach\u0026rsquo;s alpha, see example here), and can be reduced (e.g. using multiple indicators). Systematic error however, is harder to detect, harder to quantify, and harder to correct for.\nAbout the True Score Theory $T = X + \\epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$? Unfortunately, we can never be 100% sure what the value of $T$ is. As mentioned above, while we can detect and correct for random errors, systematic errors cannot be corrected using statistical procedures. After we’ve done our best to minimize random error, it is up to the strength of our theory, clarity of conceptualization, and a small leap of faith to convince others (and ourselves), that our measures are indeed valid ones. This is also part of the reason why social science research can only establish a probabilistic relationship (confident within a certain range) and never a deterministic relationship. Embrace the uncertainty!\n  If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors? Although every single indicator would be measured with some random errors, if we combine the multiple indicators as an index, or take the average value, we should have lower random errors compared to using a single indicator.\nMeasurement Reliability and Validity Is there a good analogy to help remembering the difference between validity and reliability? In class, I\u0026rsquo;ve made the analogy comparing a valid indicator as a correct label (indicator) matching the content of a box (concept) that you wanted to buy but cannot see what is inside.\n  Houston, we have a invalid indicator problem.   I don\u0026rsquo;t really have a good one for reliability, so let\u0026rsquo;s stretch the same label-on-a-box analogy a bit further. Suppose we have a machine printing the label for the box, although the label correctly matches the box content (valid indicator), the machine sometimes misprints a letter or two, so not all labels look the same (unreliable). And if we have Machine A that produces 5% misprinted labels, and Machine B that produces 15% misprinted labels, then we can say that B is less reliable (produces less consistent outcomes).\nWhat are some examples of face validity? Whenever you see a indicator used to measure a particular concept, simply ask yourself: does the measure appear to capture the concept you care about? If yes, then the measure has high face validity; if not, then it has low face validity.\nSay I want to measure whether a country\u0026rsquo;s level of human rights protection, which of the following indicators has a higher face validity?\n Gini coefficient Number of political imprisonment  You probably have an answer in your mind. Let\u0026rsquo;s try another one: now I want to measure a country\u0026rsquo;s income inequality, which indicator has a higher face validity?\n Gini coefficient Number of political imprisonment  Again, you have an answer, and you are probably right.\nA few things I\u0026rsquo;d like to highlight from this example:\n Assessing face validity relies on domain knowledge. We need to first know what \u0026ldquo;human rights protection\u0026rdquo; means, only then we can see that more political imprisonment is an indicator for low levels of human rights protection. Assessing face validity is largely based on judgment based on domain knowledge, rather than empirical demonstration. Indicator validity is always assessed relative to the concept we are trying to capture, rather than something inherent to the indicator itself. Gini coefficient is a valid indicator for income inequality, but not human rights protection.  When do we test for construct vs face validity? Ideally both, and more if possible. Since having invalid measures are really bad news, assessing the validity of a measure in multiple ways would increase the confidence\nFace validity is rarely explicitly tested for \u0026ndash; we already implicitly test for face validity when we are making the choice of which indicators to use to measure the concept. Although having face validity is important, high face validity alone is a rather weak evidence.\nConstruct validity can be empirically assessed in two ways: convergent validity and divergent validity. See here for an example.\nGenerally, if we are using the measures that have been used in published literature, we do not have to conduct separate validity test. The assumption is that they have already been previously validated (though we should still remain critical). If we are using new measures in our study, instead of established ones used in published literature, then it is recommended to first conduct a pilot study to test the measure\u0026rsquo;s validity and reliability. Use the measures as part of the actual study only after we know it\u0026rsquo;s valid and reliable.\nAbout the article we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable? For IAT, the actual computation of the score takes quite a few steps, but to simplify it a bit, it is the reaction time differential that is used as a measure of implicit racial bias (see the test procedure here). So in this case, the test can be considered as reliable if respondent has the same directional preference (e.g. consistently faster at White-Pleasant association, than Black-Pleasant association) when taking the test multiple times.\nFor other tests however, it could be the case that time difference itself, rather than directional difference is used as the measure.\nIn general, test-retest reliability is measured as degree of correlation between the different test scores, rather than absolute difference. In psychology, rule of thumb is that test-retest reliability \u0026gt; 0.7 is an acceptable level, though this is no more than a convention used by researchers. IAT has a test-retest reliability of about 0.6.\nThis Podcast has a pretty interesting discussion on the use and critique of IAT.\nLevels of Measurement Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures? A variable with meaningful zero point means that we can interpret the zero value as the absence of that variable. For example, income measured in dollars has a meaningful zero — we can interpret income = 0 to mean an absence of income. So if someone reported zero on this measure, we know this person has no income.\nOn the other hand, if the variable has a relative, or arbitrary zero points, we cannot interpret the zero value on that variable as the absence of that variable. Say we have a set of 5 questions to measure people’s political knowledge. Every correct answer gets you 1 point, and every wrong answer gets you 0 point, which gives us a range of possible score from 0 to 5. If Ann gets score = 0 on this scale, we cannot say that Ann has no political knowledge at all. The zero here is simply an arbitrary point to signal a very low level of political knowledge.\nSo how does this relate to interval vs ratio measures? Interval measures have relative/arbitrary zero points, and ratio measures have absolute/meaningful zero points. For the most part, the difference is only apparent (or we only need to pay attention to the difference) when we analyze and interpret the data.\nFor interval measures, since the zero point is arbitrary and lacks any meaningful interpretation, we cannot compare any differences in terms of proportion. It only make sense to compare the difference in magnitude. Going back to the political knowledge example, if Beth gets score = 2 on the political scale, and Cathy gets score = 4 on the same scale, we know that: 1) Cathy is more knowledgeable than Beth, and 2) the magnitude of difference is 2 more correct answers. However, since the zero point is arbitrary in this case, we cannot say Cathy is two times more knowledgeable than Beth. Or if we observe that Beth\u0026rsquo;s score increased from 2 to 3 after attending a civics education workshop, we cannot cay that Beth\u0026rsquo;s political knowledge increased by 50%.\nLet\u0026rsquo;s compare to a ratio scale, income, which has a meaningful zero point. If Abe reported income = 20k, and Ben reported income = 40k, we know that 1) Ben has higher income than Abe, 2) Ben\u0026rsquo;s income is 20k higher than Abe, and 3) that Ben has an income twice as much as Abe.\nCan a measure be both interval and ratio? The four levels of measurement are mutually exclusive categories. The flow chart below should help you to distinguish the four categories.\n  ''as-if''  random?\"] -- yes --- A1 A -- no --- A2[Are the researchers directly manipulating the treatment?] A1[Are the researchers directly manipulating the treatment?] -- yes --- B1[Randomized experiment] A1 -- no --- B2[Natural experiment] A2 -- yes --- C1[Quasi-experiment] A2 -- no --- C2[Observational design: Do we have single or repeated measures for the same unit?] C2 -- single --- D1[Cross-sectional] C2 -- repeated --- D2[Longitudinal: Panel / Time-series] ``` -- ","date":1547787600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"5afe73338ff35f4df33677cf56804b78","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/","publishdate":"2019-01-18T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-18-measurement/","section":"post","summary":"Q\u0026A on measurement errors, measurement validity and reliability, and levels of measurement","tags":["Measurement"],"title":"Q\u0026A Week 2: Measurement","type":"post"}]