<<<<<<< HEAD
[{"authors":null,"categories":null,"content":"To be updated\n Setting up R and RStudio Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","date":1548738000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1549515600,"objectID":"389d456065c47b7be0bc1c147dc1b25d","permalink":"https://fanghuiz.github.io/ps0700/tutorial_r/","publishdate":"2019-01-29T00:00:00-05:00","relpermalink":"/ps0700/tutorial_r/","section":"tutorial_r","summary":"To be updated\n Setting up R and RStudio Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"To be updated\n Setting up Stata Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","date":1548738000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1549515600,"objectID":"3e030c29ad53ebb0c0407986b4254cb1","permalink":"https://fanghuiz.github.io/ps0700/tutorial_stata/","publishdate":"2019-01-29T00:00:00-05:00","relpermalink":"/ps0700/tutorial_stata/","section":"tutorial_stata","summary":"To be updated\n Setting up Stata Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Table of Contents    About Informed Consent    Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment? Does knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome?   About the Montana GOTV Experiment    The Montana experiment misled the people by using official seal. How did they get the project approved in the first place? How did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome.   About Experiments on Development Programs    Are there examples of ethical and effective anti-poverty experiments? Not a question, just an interesting observation \u0026ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate.      About Informed Consent Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment? Yes, informed consent is an essential element of research ethics.\nGenerally speaking, we have to inform the participants the purpose of our research (e.g. “This is a study about attitudes towards political candidates), though we do not have to tell them the exact hypothesis of the study.\nIt is also important that the informed consent form has to let the participants know if there is any potential benefits or harms by taking part in the study, any compensations or incentives, confidentiality or privacy of the data, their rights to decline and to withdraw, so they can make an informed decision about participating. In most cases, political science experiments only involve \u0026ldquo;minimal risks\u0026rdquo;, i.e. about the same probability and magnitude of harm we would experience in daily life.\nFor more details on the important elements to include when obtaining informed consent, see this guide from Pitt IRB, or American Psychological Association (APA) ethics code (Section 8.02). It is possible to request waivers with adequate justification (see here for an overview of the requirements).\nDoes knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome? Quite likely! One possibility is Hawthorne effect: simply being part of the experiment and knowing that you are being observed might change your behavior or how you respond, compare to everyday life scenario.\nA more general phenomenon (some argue subsumes the Hawthorne effect) is called demand characteristics (also see textbook p.178), referring to how participants\u0026rsquo; interpretation of the experiment\u0026rsquo;s purpose could potentially change their behaviors (e.g. behave in ways conforming to what they think the researchers want to observe, or they might behave in ways contradicting to what they perceived as the researchers\u0026rsquo; hypothesis).\nIt is worth noting however, that not all experiments are equally affected by this potential problem. We might expect that experiments looking at behaviors that are more susceptible to social desirability bias are more vulnerable to bias introduced by demand characteristics, than those looking at more benign phenomenons.\nWhile it is difficult to eliminate this effect completely in most experiments, some strategies exist. For example, researchers can devise a design that uses covert or unobtrusive treatments, so the participants are unaware that they are part of an experiment (e.g. Enos 2014, Sands 2017).\nDeception is another common, though deeply controversial strategy. For example audit experiments often rely on deception to examine socially undesirable behaviors such as discrimination (e.g. Butler and Broockman 2011), norms or rules violation (e.g. Findley, Neilson and Sharman 2014).\nOf course, the use of deception always has to be justified in the ethics review process. See this newsletter (p.13-19) for further discussion on the ethics of using deception in field experiment involving public officials as subjects.\nAbout the Montana GOTV Experiment The Montana experiment misled the people by using official seal. How did they get the project approved in the first place? Only the people involved in the process would ever know! If I were to hazard a guess (take it with many many grains of salt), it is possible that the review process did not see the mailer as being intentionally misleading. Among the commotion in the follow-up to this controversy, one detail about the mailer did not get much attention — there was in fact a disclaimer line disclosing that the mailer is part of a academic study (below the boxes indicating candidate ideology).\n  Mailer from the experiment. Squint a little to see the disclaimer. Retrieved from Internet Archive    Maybe it’s too much of a fine print, but it’s there. So you could make the argument that they are not actively trying to deceive the recipient about who is sending the mailer out, and this might be part of reason why the proposal was approved. Again, I have to emphasize that this is all speculations on my part.\nHow did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome. We might never know! After the whole debacle, the study is unpublishable. Partly due to the ethical issue, partly because the data is likely unusable, given the spillover/contamination effect caused by the news coverage. After the news outlets reported about the story, those in the treatment group who have received the mailer would have known about where this mailer comes from and why they are receiving it (treatment is contaminated by extraneous factors that the researchers did not intend to provide), and those in the control group would also have known about the information in the mailer despite not receiving one (treatment spillover).\nAbout Experiments on Development Programs Are there examples of ethical and effective anti-poverty experiments? There are many examples of using randomized experiments to evaluate the impacts of anti-poverty programs. Some good places to look for them: Poverty Action Lab (J-PAL) (research center at MIT), GiveWell (nonprofit focused on effective charities).\nNot a question, just an interesting observation \u0026ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate. Hmm, this is really interesting to know. So if the experiment shows that UBI improves some aspects of life quality (e.g. household income, children\u0026rsquo;s education), but also has other \u0026ldquo;side-effects\u0026rdquo; such as increases divorce rate, from the policy-makers\u0026rsquo; position, what should they make of this? What kind of \u0026ldquo;side-effects\u0026rdquo;, or how much, would be considered as a \u0026ldquo;reasonable\u0026rdquo; level of trade-off? Back to what we discussed in the beginning of the course, empirical evidence does not always lead to a neat solution to normative questions.\n","date":1548392400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"42af2b4d2044a2804ed2adc2e35dc482","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-25-experiment/","publishdate":"2019-01-25T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-25-experiment/","section":"post","summary":"Q\u0026A on informed consent in research, the controversial Montana GOTV experiment, and experiments on development programs","tags":["Experiments","Ethics"],"title":"Q\u0026A Week 3: Experiments and Ethics","type":"post"},{"authors":[],"categories":null,"content":" Example: Racial Resentment Scale Racial resentment scale is commonly used to measure symbolic racism. The scale contains four items, for each question, respondents indicate whether they agree or disagree with the statement on a five-point scale. The question wording and the respective variable number as appeared in American National Election Studies (ANES) 2016 are given below:\n V162211: \u0026lsquo;Irish, Italians, Jewish and many other minorities overcame prejudice and worked their way up. Blacks should do the same without any special favors.\u0026rsquo; V162212: \u0026lsquo;Generations of slavery and discrimination have created conditions that make it difficult for blacks to work their way out of the lower class.\u0026rsquo; V162213: \u0026lsquo;Over the past few years, blacks have gotten less than they deserve.\u0026rsquo; V162214: \u0026lsquo;It’s really a matter of some people not trying hard enough, if blacks would only try harder they could be just as well off as whites.\u0026rsquo;  The assumption is that agreeing with statement 1 and 4 (or disagreeing with statement 2 and 3) are indications of resentment towards African Americans.\nValidity Test Construct validity To test for construct validity, we need to demonstrate that the indicator predicts what it is supposed to predict.\nOne aspect of construct validity is convergent validity: if theoretically we expect X and Y to be positively related, do we see a positive correlation between the indicator for X and Y?\nIn this case, theoretically we might expect that feelings of resentment towards African Americans would correlate with negative affective attitudes towards the group.\nFor illustration purposes, let\u0026rsquo;s just use a single statement from the resentment scale, statement 2 (V162212) about the effects of slavery and see if people\u0026rsquo;s answer to this questions correlates with their feeling thermometer score towards Blacks (V162312).\n. twoway (scatter V162312 V162212) (lfit V162312 V162212)   --   We see that higher disagreement with the statement (1 = Strongly Agree, 5 = Strongly Disagree) correlates with lower scores on the feeling thermometer (higher value means warmer feeling towards the group, lower value means colder feeling). Resentment towards African Americans (as indicated by denying the effects of slavery on their current day hardship) indeed predicts a more negative attitudes towards them (as indicated by expressing less warm feelings).\nAnother way to demonstrate construct validity is to show divergent/discriminant validity: if theoretically we do not expect X and Y to be related, do we then see a low or weak correlation between them?\nFor instance, perhaps we do not expect feelings of racial resentment to be correlated with feelings towards the Supreme Court (V162102).\n. graph twoway (scatter V162102 V162212) (lfit V162102 V162212)   --   We see that there is no discernible correlation between responses to statement 2 and feelings towards the Supreme Court.\nReliability Test One common way to quantify the reliability of a multiple indicator scale is to calculate the Cronbach\u0026rsquo;s alpha $\\alpha$.\nThis can be done in Stata using a simple command alpha, followed by the list of variables used in the scale.\n. alpha V162211 V162212 V162213 V162214 Test scale = mean(unstandardized items) Reversed items: V162211 V162214 Average interitem covariance: 1.090995 Number of items in the scale: 4 Scale reliability coefficient: 0.8451  In the output, we can see a \u0026ldquo;Scale reliability coefficient\u0026rdquo;, which is ~0.8 in this case. A general rule of thumb is that \u0026gt; 0.8 indicates rather high reliability, and anything below 0.7 is a sign of unreliable scale. So in this case, the four-item Racial Resentment Scale has rather high reliability.\n","date":1547874000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"92651ff548424570f1150e6478abb4ac","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/","publishdate":"2019-01-19T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-19-example-measurement-stata/","section":"post","summary":"Simple demonstration of reliability and validity tests in Stata using racial resentment scale as an example","tags":["Measurement","Stata"],"title":"Example: Testing for measurement validity and reliability","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    Types of measurement Errors    How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement? Which error (systematic vs random) is worse? Which one should we try to avoid more? About the True Score Theory $T = X + \\epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$? If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?   Measurement Reliability and Validity    Is there a good analogy to help remembering the difference between validity and reliability? What are some examples of face validity? When do we test for construct vs face validity? About the article we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?   Levels of Measurement    Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures? Can a measure be both interval and ratio?      Types of measurement Errors How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement? Systematic errors affects all units in the sample in the same direction (all measured values are consistently more positive or more negative than true value). For example, self-reported measure of turnout is likely to have positive systematic error \u0026ndash; (most) people tend to over-report, saying that they have voted even though they have not.\nRandom errors do not affect all units in the sample in a consistent way \u0026ndash; some units will be more positive than true value, some units will be more negative than true value. Let\u0026rsquo;s use the self-reported turnout as an example again. Perhaps people\u0026rsquo;s transient feelings about the current election affect whether they are likely to say they have voted or not \u0026ndash; those who happened to read a positive news story about the election are more likely to over-report having voted, and others who happened to read a negative news story are more likely to under-report.\nVery often both types of errors could be present, so we need to think carefully about the sources of potential errors. For example, crime statistics can be very noisy, with a lot of random errors introduced at various stages of collecting such data. Furthermore, statistics on certain types of crimes might additionally have systematic errors: for example, domestic abuse might be systematically biased downwards if victims under-report due to fear of retaliation.\nWhich error (systematic vs random) is worse? Which one should we try to avoid more? Both types of errors are bad news! But they affect our analysis in different ways.\nHigh random errors will add more noise/variability to our data, which will make it harder to detect the presence of a significant correlation between X and Y. In another word, noisy measures are bad because it increases the likelihood of false negative \u0026ndash; we are likely to mistakenly infer there is no relationship between X and Y, when in fact there is.\nFor systematic errors, recall that indicators with high systematic errors are invalid, i.e. they are not capturing the concept of interest accurately. In such case, an invalid indicator will never lead us to the right conclusion (think of a road sign that points to the wrong direction), even if the indicator is measured with zero random error.\n  One of them has got to be invalid..   In terms of which type of error is worse, one way I think about this is that invalid indicator is more like a fatal disease, and unreliable indicator is more like a non-fatal but chronic disease that requires lots of care. So if a study is using invalid indicators, we cannot draw any meaningful inferences about the phenomenon we are investigating (the study is \u0026ldquo;dead\u0026rdquo;), while unreliable indicators make it harder to detect a true positive (increases uncertainty, but does not spell doom).\nThe textbook also has a good discussion on the different problems associated with measurement reliability and validity in political science (p.143-145).\n    Systematic error = High Systematic error = Low     Random error = High Very very bad \u0026bull; Invalid and unreliable measure \u0026bull; Lots of noise, and signal is pointing at the wrong direction Problematic, but can live with \u0026bull; Valid, but unreliable measure \u0026bull; Lots of noise, harder to detect the signal; More likely to get false negative   Random error = Low Problematic \u0026bull; Invalid, but reliable measure \u0026bull; Measure does not capture the concept of interests; Conclusion does not bear on the actual phenomenon of interest Awesome! \u0026bull; Valid and reliable measure \u0026bull; Move along    Ideally, we should try to minimize both types of measurement errors. Degree of random error can be empirically assessed (e.g. using Cronbach\u0026rsquo;s alpha, see example here), and can be reduced (e.g. using multiple indicators). Systematic error however, is harder to detect, harder to quantify, and harder to correct for.\nAbout the True Score Theory $T = X + \\epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$? Unfortunately, we can never be 100% sure what the value of $T$ is. As mentioned above, while we can detect and correct for random errors, systematic errors cannot be corrected using statistical procedures. After we’ve done our best to minimize random error, it is up to the strength of our theory, clarity of conceptualization, and a small leap of faith to convince others (and ourselves), that our measures are indeed valid ones. This is also part of the reason why social science research can only establish a probabilistic relationship (confident within a certain range) and never a deterministic relationship. Embrace the uncertainty!\n  If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors? Although every single indicator would be measured with some random errors, if we combine the multiple indicators as an index, or take the average value, we should have lower random errors compared to using a single indicator.\nMeasurement Reliability and Validity Is there a good analogy to help remembering the difference between validity and reliability? In class, I\u0026rsquo;ve made the analogy comparing a valid indicator as a correct label (indicator) matching the content of a box (concept) that you wanted to buy but cannot see what is inside.\n  Houston, we have a invalid indicator problem.   I don\u0026rsquo;t really have a good one for reliability, so let\u0026rsquo;s stretch the same label-on-a-box analogy a bit further. Suppose we have a machine printing the label for the box, although the label correctly matches the box content (valid indicator), the machine sometimes misprints a letter or two, so not all labels look the same (unreliable). And if we have Machine A that produces 5% misprinted labels, and Machine B that produces 15% misprinted labels, then we can say that B is less reliable (produces less consistent outcomes).\nWhat are some examples of face validity? Whenever you see a indicator used to measure a particular concept, simply ask yourself: does the measure appear to capture the concept you care about? If yes, then the measure has high face validity; if not, then it has low face validity.\nSay I want to measure whether a country\u0026rsquo;s level of human rights protection, which of the following indicators has a higher face validity?\n Gini coefficient Number of political imprisonment  You probably have an answer in your mind. Let\u0026rsquo;s try another one: now I want to measure a country\u0026rsquo;s income inequality, which indicator has a higher face validity?\n Gini coefficient Number of political imprisonment  Again, you have an answer, and you are probably right.\nA few things I\u0026rsquo;d like to highlight from this example:\n Assessing face validity relies on domain knowledge. We need to first know what \u0026ldquo;human rights protection\u0026rdquo; means, only then we can see that more political imprisonment is an indicator for low levels of human rights protection. Assessing face validity is largely based on judgment based on domain knowledge, rather than empirical demonstration. Indicator validity is always assessed relative to the concept we are trying to capture, rather than something inherent to the indicator itself. Gini coefficient is a valid indicator for income inequality, but not human rights protection.  When do we test for construct vs face validity? Ideally both, and more if possible. Since having invalid measures are really bad news, assessing the validity of a measure in multiple ways would increase the confidence\nFace validity is rarely explicitly tested for \u0026ndash; we already implicitly test for face validity when we are making the choice of which indicators to use to measure the concept. Although having face validity is important, high face validity alone is a rather weak evidence.\nConstruct validity can be empirically assessed in two ways: convergent validity and divergent validity. See here for an example.\nGenerally, if we are using the measures that have been used in published literature, we do not have to conduct separate validity test. The assumption is that they have already been previously validated (though we should still remain critical). If we are using new measures in our study, instead of established ones used in published literature, then it is recommended to first conduct a pilot study to test the measure\u0026rsquo;s validity and reliability. Use the measures as part of the actual study only after we know it\u0026rsquo;s valid and reliable.\nAbout the article we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable? For IAT, the actual computation of the score takes quite a few steps, but to simplify it a bit, it is the reaction time differential that is used as a measure of implicit racial bias (see the test procedure here). So in this case, the test can be considered as reliable if respondent has the same directional preference (e.g. consistently faster at White-Pleasant association, than Black-Pleasant association) when taking the test multiple times.\nFor other tests however, it could be the case that time difference itself, rather than directional difference is used as the measure.\nIn general, test-retest reliability is measured as degree of correlation between the different test scores, rather than absolute difference. In psychology, rule of thumb is that test-retest reliability \u0026gt; 0.7 is an acceptable level, though this is no more than a convention used by researchers. IAT has a test-retest reliability of about 0.6.\nThis Podcast has a pretty interesting discussion on the use and critique of IAT.\nLevels of Measurement Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures? A variable with meaningful zero point means that we can interpret the zero value as the absence of that variable. For example, income measured in dollars has a meaningful zero — we can interpret income = 0 to mean an absence of income. So if someone reported zero on this measure, we know this person has no income.\nOn the other hand, if the variable has a relative, or arbitrary zero points, we cannot interpret the zero value on that variable as the absence of that variable. Say we have a set of 5 questions to measure people’s political knowledge. Every correct answer gets you 1 point, and every wrong answer gets you 0 point, which gives us a range of possible score from 0 to 5. If Ann gets score = 0 on this scale, we cannot say that Ann has no political knowledge at all. The zero here is simply an arbitrary point to signal a very low level of political knowledge.\nSo how does this relate to interval vs ratio measures? Interval measures have relative/arbitrary zero points, and ratio measures have absolute/meaningful zero points. For the most part, the difference is only apparent (or we only need to pay attention to the difference) when we analyze and interpret the data.\nFor interval measures, since the zero point is arbitrary and lacks any meaningful interpretation, we cannot compare any differences in terms of proportion. It only make sense to compare the difference in magnitude. Going back to the political knowledge example, if Beth gets score = 2 on the political scale, and Cathy gets score = 4 on the same scale, we know that: 1) Cathy is more knowledgeable than Beth, and 2) the magnitude of difference is 2 more correct answers. However, since the zero point is arbitrary in this case, we cannot say Cathy is two times more knowledgeable than Beth. Or if we observe that Beth\u0026rsquo;s score increased from 2 to 3 after attending a civics education workshop, we cannot cay that Beth\u0026rsquo;s political knowledge increased by 50%.\nLet\u0026rsquo;s compare to a ratio scale, income, which has a meaningful zero point. If Abe reported income = 20k, and Ben reported income = 40k, we know that 1) Ben has higher income than Abe, 2) Ben\u0026rsquo;s income is 20k higher than Abe, and 3) that Ben has an income twice as much as Abe.\nCan a measure be both interval and ratio? The four levels of measurement are mutually exclusive categories. The flow chart below should help you to distinguish the four categories.\n  ''as-if''  random?\"] -- yes --- A1 A -- no --- A2[Are the researchers directly manipulating the treatment?] A1[Are the researchers directly manipulating the treatment?] -- yes --- B1[Randomized experiment] A1 -- no --- B2[Natural experiment] A2 -- yes --- C1[Quasi-experiment] A2 -- no --- C2[Observational design: Do we have single or repeated measures for the same unit?] C2 -- single --- D1[Cross-sectional] C2 -- repeated --- D2[Longitudinal: Panel / Time-series] ``` -- ","date":1547787600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"5afe73338ff35f4df33677cf56804b78","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/","publishdate":"2019-01-18T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-18-measurement/","section":"post","summary":"Q\u0026A on measurement errors, measurement validity and reliability, and levels of measurement","tags":["Measurement"],"title":"Q\u0026A Week 2: Measurement","type":"post"}]
=======
[{"authors":null,"categories":null,"content":"To be updated\n Setting up R and RStudio Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","date":1548738000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1549515600,"objectID":"389d456065c47b7be0bc1c147dc1b25d","permalink":"https://fanghuiz.github.io/ps0700/tutorial_r/","publishdate":"2019-01-29T00:00:00-05:00","relpermalink":"/ps0700/tutorial_r/","section":"tutorial_r","summary":"To be updated\n Setting up R and RStudio Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"To be updated\n Setting up Stata Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","date":1548738000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1549515600,"objectID":"3e030c29ad53ebb0c0407986b4254cb1","permalink":"https://fanghuiz.github.io/ps0700/tutorial_stata/","publishdate":"2019-01-29T00:00:00-05:00","relpermalink":"/ps0700/tutorial_stata/","section":"tutorial_stata","summary":"To be updated\n Setting up Stata Data manipulation Descriptive statistics Exploratory visualization Inferential statistics  ","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Table of Contents    About Informed Consent    Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment? Does knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome?   About the Montana GOTV Experiment    The Montana experiment misled the people by using official seal. How did they get the project approved in the first place? How did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome.   About Experiments on Development Programs    Are there examples of ethical and effective anti-poverty experiments? Not a question, just an interesting observation \u0026ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate.      About Informed Consent Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment? Yes, informed consent is an essential element of research ethics.\nGenerally speaking, we have to inform the participants the purpose of our research (e.g. “This is a study about attitudes towards political candidates), though we do not have to tell them the exact hypothesis of the study.\nIt is also important that the informed consent form has to let the participants know if there is any potential benefits or harms by taking part in the study, any compensations or incentives, confidentiality or privacy of the data, their rights to decline and to withdraw, so they can make an informed decision about participating. In most cases, political science experiments only involve \u0026ldquo;minimal risks\u0026rdquo;, i.e. about the same probability and magnitude of harm we would experience in daily life.\nFor more details on the important elements to include when obtaining informed consent, see this guide from Pitt IRB, or American Psychological Association (APA) ethics code (Section 8.02). It is possible to request waivers with adequate justification (see here for an overview of the requirements).\nDoes knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome? Quite likely! One possibility is Hawthorne effect: simply being part of the experiment and knowing that you are being observed might change your behavior or how you respond, compare to everyday life scenario.\nA more general phenomenon (some argue subsumes the Hawthorne effect) is called demand characteristics (also see textbook p.178), referring to how participants\u0026rsquo; interpretation of the experiment\u0026rsquo;s purpose could potentially change their behaviors (e.g. behave in ways conforming to what they think the researchers want to observe, or they might behave in ways contradicting to what they perceived as the researchers\u0026rsquo; hypothesis).\nIt is worth noting however, that not all experiments are equally affected by this potential problem. We might expect that experiments looking at behaviors that are more susceptible to social desirability bias are more vulnerable to bias introduced by demand characteristics, than those looking at more benign phenomenons.\nWhile it is difficult to eliminate this effect completely in most experiments, some strategies exist. For example, researchers can devise a design that uses covert or unobtrusive treatments, so the participants are unaware that they are part of an experiment (e.g. Enos 2014, Sands 2017).\nDeception is another common, though deeply controversial strategy. For example audit experiments often rely on deception to examine socially undesirable behaviors such as discrimination (e.g. Butler and Broockman 2011), norms or rules violation (e.g. Findley, Neilson and Sharman 2014).\nOf course, the use of deception always has to be justified in the ethics review process. See this newsletter (p.13-19) for further discussion on the ethics of using deception in field experiment involving public officials as subjects.\nAbout the Montana GOTV Experiment The Montana experiment misled the people by using official seal. How did they get the project approved in the first place? Only the people involved in the process would ever know! If I were to hazard a guess (take it with many many grains of salt), it is possible that the review process did not see the mailer as being intentionally misleading. Among the commotion in the follow-up to this controversy, one detail about the mailer did not get much attention — there was in fact a disclaimer line disclosing that the mailer is part of a academic study (below the boxes indicating candidate ideology).\n  Mailer from the experiment. Squint a little to see the disclaimer. Retrieved from Internet Archive    Maybe it’s too much of a fine print, but it’s there. So you could make the argument that they are not actively trying to deceive the recipient about who is sending the mailer out, and this might be part of reason why the proposal was approved. Again, I have to emphasize that this is all speculations on my part.\nHow did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome. We might never know! After the whole debacle, the study is unpublishable. Partly due to the ethical issue, partly because the data is likely unusable, given the spillover/contamination effect caused by the news coverage. After the news outlets reported about the story, those in the treatment group who have received the mailer would have known about where this mailer comes from and why they are receiving it (treatment is contaminated by extraneous factors that the researchers did not intend to provide), and those in the control group would also have known about the information in the mailer despite not receiving one (treatment spillover).\nAbout Experiments on Development Programs Are there examples of ethical and effective anti-poverty experiments? There are many examples of using randomized experiments to evaluate the impacts of anti-poverty programs. Some good places to look for them: Poverty Action Lab (J-PAL) (research center at MIT), GiveWell (nonprofit focused on effective charities).\nNot a question, just an interesting observation \u0026ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate. Hmm, this is really interesting to know. So if the experiment shows that UBI improves some aspects of life quality (e.g. household income, children\u0026rsquo;s education), but also has other \u0026ldquo;side-effects\u0026rdquo; such as increases divorce rate, from the policy-makers\u0026rsquo; position, what should they make of this? What kind of \u0026ldquo;side-effects\u0026rdquo;, or how much, would be considered as a \u0026ldquo;reasonable\u0026rdquo; level of trade-off? Back to what we discussed in the beginning of the course, empirical evidence does not always lead to a neat solution to normative questions.\n","date":1548392400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"42af2b4d2044a2804ed2adc2e35dc482","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-25-experiment/","publishdate":"2019-01-25T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-25-experiment/","section":"post","summary":"Q\u0026A on informed consent in research, the controversial Montana GOTV experiment, and experiments on development programs","tags":["Experiments","Ethics"],"title":"Q\u0026A Week 3: Experiments and Ethics","type":"post"},{"authors":[],"categories":null,"content":" Example: Racial Resentment Scale Racial resentment scale is commonly used to measure symbolic racism. The scale contains four items, for each question, respondents indicate whether they agree or disagree with the statement on a five-point scale. The question wording and the respective variable number as appeared in American National Election Studies (ANES) 2016 are given below:\n V162211: \u0026lsquo;Irish, Italians, Jewish and many other minorities overcame prejudice and worked their way up. Blacks should do the same without any special favors.\u0026rsquo; V162212: \u0026lsquo;Generations of slavery and discrimination have created conditions that make it difficult for blacks to work their way out of the lower class.\u0026rsquo; V162213: \u0026lsquo;Over the past few years, blacks have gotten less than they deserve.\u0026rsquo; V162214: \u0026lsquo;It’s really a matter of some people not trying hard enough, if blacks would only try harder they could be just as well off as whites.\u0026rsquo;  The assumption is that agreeing with statement 1 and 4 (or disagreeing with statement 2 and 3) are indications of resentment towards African Americans.\nValidity Test Construct validity To test for construct validity, we need to demonstrate that the indicator predicts what it is supposed to predict.\nOne aspect of construct validity is convergent validity: if theoretically we expect X and Y to be positively related, do we see a positive correlation between the indicator for X and Y?\nIn this case, theoretically we might expect that feelings of resentment towards African Americans would correlate with negative affective attitudes towards the group.\nFor illustration purposes, let\u0026rsquo;s just use a single statement from the resentment scale, statement 2 (V162212) about the effects of slavery and see if people\u0026rsquo;s answer to this questions correlates with their feeling thermometer score towards Blacks (V162312).\n. twoway (scatter V162312 V162212) (lfit V162312 V162212)  We see that higher disagreement with the statement (1 = Strongly Agree, 5 = Strongly Disagree) correlates with lower scores on the feeling thermometer (higher value means warmer feeling towards the group, lower value means colder feeling). Resentment towards African Americans (as indicated by denying the effects of slavery on their current day hardship) indeed predicts a more negative attitudes towards them (as indicated by expressing less warm feelings).\nAnother way to demonstrate construct validity is to show divergent/discriminant validity: if theoretically we do not expect X and Y to be related, do we then see a low or weak correlation between them?\nFor instance, perhaps we do not expect feelings of racial resentment to be correlated with feelings towards the Supreme Court (V162102).\n. graph twoway (scatter V162102 V162212) (lfit V162102 V162212)  We see that there is no discernible correlation between responses to statement 2 and feelings towards the Supreme Court.\nReliability Test One common way to quantify the reliability of a multiple indicator scale is to calculate the Cronbach\u0026rsquo;s alpha $\\alpha$.\nThis can be done in Stata using a simple command alpha, followed by the list of variables used in the scale.\n. alpha V162211 V162212 V162213 V162214 Test scale = mean(unstandardized items) Reversed items: V162211 V162214 Average interitem covariance: 1.090995 Number of items in the scale: 4 Scale reliability coefficient: 0.8451  In the output, we can see a \u0026ldquo;Scale reliability coefficient\u0026rdquo;, which is ~0.8 in this case. A general rule of thumb is that \u0026gt; 0.8 indicates rather high reliability, and anything below 0.7 is a sign of unreliable scale. So in this case, the four-item Racial Resentment Scale has rather high reliability.\n","date":1547874000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"92651ff548424570f1150e6478abb4ac","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/","publishdate":"2019-01-19T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-19-example-measurement-stata/","section":"post","summary":"Simple demonstration of reliability and validity tests in Stata using racial resentment scale as an example","tags":["Measurement","Stata"],"title":"Example: Testing for measurement validity and reliability","type":"post"},{"authors":[],"categories":null,"content":" Table of Contents    Types of measurement Errors    How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement? Which error (systematic vs random) is worse? Which one should we try to avoid more? About the True Score Theory $T = X + \\epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$? If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?   Measurement Reliability and Validity    Is there a good analogy to help remembering the difference between validity and reliability? What are some examples of face validity? When do we test for construct vs face validity? About the article we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?   Levels of Measurement    Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures? Can a measure be both interval and ratio?      Types of measurement Errors How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement? Systematic errors affects all units in the sample in the same direction (all measured values are consistently more positive or more negative than true value). For example, self-reported measure of turnout is likely to have positive systematic error \u0026ndash; (most) people tend to over-report, saying that they have voted even though they have not.\nRandom errors do not affect all units in the sample in a consistent way \u0026ndash; some units will be more positive than true value, some units will be more negative than true value. Let\u0026rsquo;s use the self-reported turnout as an example again. Perhaps people\u0026rsquo;s transient feelings about the current election affect whether they are likely to say they have voted or not \u0026ndash; those who happened to read a positive news story about the election are more likely to over-report having voted, and others who happened to read a negative news story are more likely to under-report.\nVery often both types of errors could be present, so we need to think carefully about the sources of potential errors. For example, crime statistics can be very noisy, with a lot of random errors introduced at various stages of collecting such data. Furthermore, statistics on certain types of crimes might additionally have systematic errors: for example, domestic abuse might be systematically biased downwards if victims under-report due to fear of retaliation.\nWhich error (systematic vs random) is worse? Which one should we try to avoid more? Both types of errors are bad news! But they affect our analysis in different ways.\nHigh random errors will add more noise/variability to our data, which will make it harder to detect the presence of a significant correlation between X and Y. In another word, noisy measures are bad because it increases the likelihood of false negative \u0026ndash; we are likely to mistakenly infer there is no relationship between X and Y, when in fact there is.\nFor systematic errors, recall that indicators with high systematic errors are invalid, i.e. they are not capturing the concept of interest accurately. In such case, an invalid indicator will never lead us to the right conclusion (think of a road sign that points to the wrong direction), even if the indicator is measured with zero random error.\n  One of them has got to be invalid..   In terms of which type of error is worse, one way I think about this is that invalid indicator is more like a fatal disease, and unreliable indicator is more like a non-fatal but chronic disease that requires lots of care. So if a study is using invalid indicators, we cannot draw any meaningful inferences about the phenomenon we are investigating (the study is \u0026ldquo;dead\u0026rdquo;), while unreliable indicators make it harder to detect a true positive (increases uncertainty, but does not spell doom).\nThe textbook also has a good discussion on the different problems associated with measurement reliability and validity in political science (p.143-145).\n    Systematic error = High Systematic error = Low     Random error = High Very very bad \u0026bull; Invalid and unreliable measure \u0026bull; Lots of noise, and signal is pointing at the wrong direction Problematic, but can live with \u0026bull; Valid, but unreliable measure \u0026bull; Lots of noise, harder to detect the signal; More likely to get false negative   Random error = Low Problematic \u0026bull; Invalid, but reliable measure \u0026bull; Measure does not capture the concept of interests; Conclusion does not bear on the actual phenomenon of interest Awesome! \u0026bull; Valid and reliable measure \u0026bull; Move along    Ideally, we should try to minimize both types of measurement errors. Degree of random error can be empirically assessed (e.g. using Cronbach\u0026rsquo;s alpha, see example here), and can be reduced (e.g. using multiple indicators). Systematic error however, is harder to detect, harder to quantify, and harder to correct for.\nAbout the True Score Theory $T = X + \\epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$? Unfortunately, we can never be 100% sure what the value of $T$ is. As mentioned above, while we can detect and correct for random errors, systematic errors cannot be corrected using statistical procedures. After we’ve done our best to minimize random error, it is up to the strength of our theory, clarity of conceptualization, and a small leap of faith to convince others (and ourselves), that our measures are indeed valid ones. This is also part of the reason why social science research can only establish a probabilistic relationship (confident within a certain range) and never a deterministic relationship. Embrace the uncertainty!\n  If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors? Although every single indicator would be measured with some random errors, if we combine the multiple indicators as an index, or take the average value, we should have lower random errors compared to using a single indicator.\nMeasurement Reliability and Validity Is there a good analogy to help remembering the difference between validity and reliability? In class, I\u0026rsquo;ve made the analogy comparing a valid indicator as a correct label (indicator) matching the content of a box (concept) that you wanted to buy but cannot see what is inside.\n  Houston, we have a invalid indicator problem.   I don\u0026rsquo;t really have a good one for reliability, so let\u0026rsquo;s stretch the same label-on-a-box analogy a bit further. Suppose we have a machine printing the label for the box, although the label correctly matches the box content (valid indicator), the machine sometimes misprints a letter or two, so not all labels look the same (unreliable). And if we have Machine A that produces 5% misprinted labels, and Machine B that produces 15% misprinted labels, then we can say that B is less reliable (produces less consistent outcomes).\nWhat are some examples of face validity? Whenever you see a indicator used to measure a particular concept, simply ask yourself: does the measure appear to capture the concept you care about? If yes, then the measure has high face validity; if not, then it has low face validity.\nSay I want to measure whether a country\u0026rsquo;s level of human rights protection, which of the following indicators has a higher face validity?\n Gini coefficient Number of political imprisonment  You probably have an answer in your mind. Let\u0026rsquo;s try another one: now I want to measure a country\u0026rsquo;s income inequality, which indicator has a higher face validity?\n Gini coefficient Number of political imprisonment  Again, you have an answer, and you are probably right.\nA few things I\u0026rsquo;d like to highlight from this example:\n Assessing face validity relies on domain knowledge. We need to first know what \u0026ldquo;human rights protection\u0026rdquo; means, only then we can see that more political imprisonment is an indicator for low levels of human rights protection. Assessing face validity is largely based on judgment based on domain knowledge, rather than empirical demonstration. Indicator validity is always assessed relative to the concept we are trying to capture, rather than something inherent to the indicator itself. Gini coefficient is a valid indicator for income inequality, but not human rights protection.  When do we test for construct vs face validity? Ideally both, and more if possible. Since having invalid measures are really bad news, assessing the validity of a measure in multiple ways would increase the confidence\nFace validity is rarely explicitly tested for \u0026ndash; we already implicitly test for face validity when we are making the choice of which indicators to use to measure the concept. Although having face validity is important, high face validity alone is a rather weak evidence.\nConstruct validity can be empirically assessed in two ways: convergent validity and divergent validity. See here for an example.\nGenerally, if we are using the measures that have been used in published literature, we do not have to conduct separate validity test. The assumption is that they have already been previously validated (though we should still remain critical). If we are using new measures in our study, instead of established ones used in published literature, then it is recommended to first conduct a pilot study to test the measure\u0026rsquo;s validity and reliability. Use the measures as part of the actual study only after we know it\u0026rsquo;s valid and reliable.\nAbout the article we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable? For IAT, the actual computation of the score takes quite a few steps, but to simplify it a bit, it is the reaction time differential that is used as a measure of implicit racial bias (see the test procedure here). So in this case, the test can be considered as reliable if respondent has the same directional preference (e.g. consistently faster at White-Pleasant association, than Black-Pleasant association) when taking the test multiple times.\nFor other tests however, it could be the case that time difference itself, rather than directional difference is used as the measure.\nIn general, test-retest reliability is measured as degree of correlation between the different test scores, rather than absolute difference. In psychology, rule of thumb is that test-retest reliability \u0026gt; 0.7 is an acceptable level, though this is no more than a convention used by researchers. IAT has a test-retest reliability of about 0.6.\nThis Podcast has a pretty interesting discussion on the use and critique of IAT.\nLevels of Measurement Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures? A variable with meaningful zero point means that we can interpret the zero value as the absence of that variable. For example, income measured in dollars has a meaningful zero — we can interpret income = 0 to mean an absence of income. So if someone reported zero on this measure, we know this person has no income.\nOn the other hand, if the variable has a relative, or arbitrary zero points, we cannot interpret the zero value on that variable as the absence of that variable. Say we have a set of 5 questions to measure people’s political knowledge. Every correct answer gets you 1 point, and every wrong answer gets you 0 point, which gives us a range of possible score from 0 to 5. If Ann gets score = 0 on this scale, we cannot say that Ann has no political knowledge at all. The zero here is simply an arbitrary point to signal a very low level of political knowledge.\nSo how does this relate to interval vs ratio measures? Interval measures have relative/arbitrary zero points, and ratio measures have absolute/meaningful zero points. For the most part, the difference is only apparent (or we only need to pay attention to the difference) when we analyze and interpret the data.\nFor interval measures, since the zero point is arbitrary and lacks any meaningful interpretation, we cannot compare any differences in terms of proportion. It only make sense to compare the difference in magnitude. Going back to the political knowledge example, if Beth gets score = 2 on the political scale, and Cathy gets score = 4 on the same scale, we know that: 1) Cathy is more knowledgeable than Beth, and 2) the magnitude of difference is 2 more correct answers. However, since the zero point is arbitrary in this case, we cannot say Cathy is two times more knowledgeable than Beth. Or if we observe that Beth\u0026rsquo;s score increased from 2 to 3 after attending a civics education workshop, we cannot cay that Beth\u0026rsquo;s political knowledge increased by 50%.\nLet\u0026rsquo;s compare to a ratio scale, income, which has a meaningful zero point. If Abe reported income = 20k, and Ben reported income = 40k, we know that 1) Ben has higher income than Abe, 2) Ben\u0026rsquo;s income is 20k higher than Abe, and 3) that Ben has an income twice as much as Abe.\nCan a measure be both interval and ratio? The four levels of measurement are mutually exclusive categories. The flow chart below should help you to distinguish the four categories.\n  ''as-if''  random?\"] -- yes --- A1 A -- no --- A2[Are the researchers directly manipulating the treatment?] A1[Are the researchers directly manipulating the treatment?] -- yes --- B1[Randomized experiment] A1 -- no --- B2[Natural experiment] A2 -- yes --- C1[Quasi-experiment] A2 -- no --- C2[Observational design: Do we have single or repeated measures for the same unit?] C2 -- single --- D1[Cross-sectional] C2 -- repeated --- D2[Longitudinal: Panel / Time-series] ``` -- ","date":1547787600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548738000,"objectID":"5afe73338ff35f4df33677cf56804b78","permalink":"https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/","publishdate":"2019-01-18T00:00:00-05:00","relpermalink":"/ps0700/post/2019-01-18-measurement/","section":"post","summary":"Q\u0026A on measurement errors, measurement validity and reliability, and levels of measurement","tags":["Measurement"],"title":"Q\u0026A Week 2: Measurement","type":"post"}]
>>>>>>> parent of 15eb630... Deleted docs folder
