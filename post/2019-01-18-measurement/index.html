<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.3.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Research Methods in Political Science">

  
  
  
  
    
  
  <meta name="description" content="Q&amp;A on measurement errors, measurement validity and reliability, and levels of measurement">

  
  <link rel="alternate" hreflang="en-us" href="https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#40a990">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/ps0700/styles.css">
  
  <link rel="stylesheet" href="/ps0700/css/custom.css">
  

  
  
  

  
  <link rel="alternate" href="https://fanghuiz.github.io/ps0700/index.xml" type="application/rss+xml" title="Political Science Research Methods">
  <link rel="feed" href="https://fanghuiz.github.io/ps0700/index.xml" type="application/rss+xml" title="Political Science Research Methods">
  

  <link rel="manifest" href="/ps0700/site.webmanifest">
  <link rel="icon" type="image/png" href="/ps0700/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/ps0700/img/icon-192.png">

  <link rel="canonical" href="https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Political Science Research Methods">
  <meta property="og:url" content="https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/">
  <meta property="og:title" content="Q&amp;A Week 2: Measurement | Political Science Research Methods">
  <meta property="og:description" content="Q&amp;A on measurement errors, measurement validity and reliability, and levels of measurement"><meta property="og:image" content="https://fanghuiz.github.io/ps0700/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-01-18T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2019-01-29T00:00:00-05:00">
  

  

  

  <title>Q&amp;A Week 2: Measurement | Political Science Research Methods</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/ps0700/">Political Science Research Methods</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/ps0700/post/">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/ps0700/tutorial_stata/">
            
            <span>Stata Tutorials</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/ps0700/tutorial_r/">
            
            <span>R Tutorials</span>
            
          </a>
        </li>

        
        

      

        

        
        
        

        <li class="nav-item">
          <a class="nav-link" href="/ps0700/">
          
          <span><i class="fab fa-github" style="font-size: 1rem; line-height: 1.25"></i></span>
          
          </a>
        </li>

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Q&amp;A Week 2: Measurement</h1>

  

  
    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Research Methods in Political Science">
  </span>
  

  <span class="article-date">
    
        Last updated on
    
    <meta content="2019-01-18 00:00:00 -0500 EST" itemprop="datePublished">
    <time datetime="2019-01-29 00:00:00 -0500 EST" itemprop="dateModified">
      Jan 29, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Research Methods in Political Science">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/ps0700/post/2019-01-18-measurement/#disqus_thread"></a>
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Q%26A%20Week%202%3a%20Measurement&amp;url=https%3a%2f%2ffanghuiz.github.io%2fps0700%2fpost%2f2019-01-18-measurement%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2ffanghuiz.github.io%2fps0700%2fpost%2f2019-01-18-measurement%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffanghuiz.github.io%2fps0700%2fpost%2f2019-01-18-measurement%2f&amp;title=Q%26A%20Week%202%3a%20Measurement"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2ffanghuiz.github.io%2fps0700%2fpost%2f2019-01-18-measurement%2f&amp;title=Q%26A%20Week%202%3a%20Measurement"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Q%26A%20Week%202%3a%20Measurement&amp;body=https%3a%2f%2ffanghuiz.github.io%2fps0700%2fpost%2f2019-01-18-measurement%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    







  









  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<h2>Table of Contents</h2>
<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#types-of-measurement-errors">Types of measurement Errors</a>
<ul>
<li>
<ul>
<li><a href="#how-to-differentiate-between-systematic-vs-random-error-do-you-have-any-examples-of-systematic-errors-in-measurement">How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement?</a></li>
<li><a href="#which-error-systematic-vs-random-is-worse-which-one-should-we-try-to-avoid-more">Which error (systematic vs random) is worse? Which one should we try to avoid more?</a></li>
<li><a href="#about-the-true-score-theory-t-x-epsilon-how-do-we-know-how-close-our-measured-value-x-is-close-to-the-true-score-t-if-we-cannot-truly-know-t">About the True Score Theory $T = X + \epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$?</a></li>
<li><a href="#if-all-indicators-are-measured-with-some-degrees-of-random-errors-can-too-many-indicators-introduce-more-random-errors">If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?</a></li>
</ul></li>
</ul></li>
<li><a href="#measurement-reliability-and-validity">Measurement Reliability and Validity</a>
<ul>
<li>
<ul>
<li><a href="#is-there-a-good-analogy-to-help-remembering-the-difference-between-validity-and-reliability">Is there a good analogy to help remembering the difference between validity and reliability?</a></li>
<li><a href="#what-are-some-examples-of-face-validity">What are some examples of face validity?</a></li>
<li><a href="#when-do-we-test-for-construct-vs-face-validity">When do we test for construct vs face validity?</a></li>
<li><a href="#about-the-article-https-www-nytimes-com-2014-08-28-opinion-nicholas-kristof-is-everyone-a-little-bit-racist-html-we-read-on-using-iat-video-games-to-measure-implicit-racial-bias-how-is-the-reliability-of-the-measure-determined-if-the-same-respondent-takes-the-test-twice-and-gets-different-scores-but-in-the-same-direction-e-g-at-first-longer-then-shorter-time-is-the-measure-considered-reliable">About the <a href="https://www.nytimes.com/2014/08/28/opinion/nicholas-kristof-is-everyone-a-little-bit-racist.html" target="_blank">article</a> we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?</a></li>
</ul></li>
</ul></li>
<li><a href="#levels-of-measurement">Levels of Measurement</a>
<ul>
<li>
<ul>
<li><a href="#can-you-elaborate-more-on-meaningful-vs-relative-arbitrary-zero-point-and-how-that-relates-to-interval-and-ratio-measures">Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures?</a></li>
<li><a href="#can-a-measure-be-both-interval-and-ratio">Can a measure be both interval and ratio?</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>


<h2 id="types-of-measurement-errors">Types of measurement Errors</h2>

<h4 id="how-to-differentiate-between-systematic-vs-random-error-do-you-have-any-examples-of-systematic-errors-in-measurement">How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement?</h4>

<p>Systematic errors affects all units in the sample in the same direction (all measured values are consistently more positive or more negative than true value). For example, self-reported measure of turnout is likely to have positive systematic error &ndash; (most) people tend to over-report, saying that they have voted even though they have not.</p>

<p>Random errors do not affect all units in the sample in a consistent way &ndash; some units will be more positive than true value, some units will be more negative than true value. Let&rsquo;s use the self-reported turnout as an example again. Perhaps people&rsquo;s transient feelings about the current election affect whether they are likely to say they have voted or not &ndash; those who happened to read a positive news story about the election are more likely to over-report having voted, and others who happened to read a negative news story are more likely to under-report.</p>

<p>Very often both types of errors could be present, so we need to think carefully about the sources of potential errors. For example, crime statistics can be very noisy, with a lot of random errors introduced at various stages of collecting such data. Furthermore, statistics on certain <em>types</em> of crimes might additionally have systematic errors: for example, domestic abuse might be systematically biased downwards if victims under-report due to fear of retaliation.</p>

<!-- As the sample size increases, we are likely to see that there are as many positive errors as negative ones, and all the random errors would sum to 0. This means that random errors add *variability/noise* to the data, but will not affect the average value at the *group* level. -->

<h4 id="which-error-systematic-vs-random-is-worse-which-one-should-we-try-to-avoid-more">Which error (systematic vs random) is worse? Which one should we try to avoid more?</h4>

<p>Both types of errors are bad news! But they affect our analysis in different ways.</p>

<p>High random errors will add more <em>noise/variability</em> to our data, which will make it harder to detect the presence of a significant correlation between X and Y. In another word, noisy measures are bad because it increases the likelihood of <em>false negative</em> &ndash; we are likely to mistakenly infer there is no relationship between X and Y, when in fact there is.</p>

<p>For systematic errors, recall that indicators with high systematic errors are <em>invalid</em>, i.e. they are not capturing the concept of interest accurately. In such case, an invalid indicator will <em>never</em> lead us to the right conclusion (think of a road sign that points to the wrong direction), even if the indicator is measured with zero random error.</p>




<figure>

<img src="https://vignette.wikia.nocookie.net/thebige/images/d/da/Funny_road_signs_5.jpg/revision/latest/scale-to-width-down/180?cb=20130807005626" alt="One of them has got to be invalid.." />



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    One of them has got to be invalid..
    
    
    
  </p> 
</figcaption>

</figure>

<p>In terms of which type of error is <em>worse</em>, one way I think about this is that invalid indicator is more like a fatal disease, and unreliable indicator is more like a non-fatal but chronic disease that requires lots of care. So if a study is using invalid indicators, we cannot draw any meaningful inferences about the phenomenon we are investigating (the study is &ldquo;dead&rdquo;), while unreliable indicators make it harder to detect a <em>true positive</em> (increases uncertainty, but does not spell doom).</p>

<p>The textbook also has a good discussion on the different problems associated with measurement reliability and validity in political science (p.143-145).</p>

<table>
<thead>
<tr>
<th></th>
<th>Systematic error = High</th>
<th>Systematic error = Low</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Random error = High</strong></td>
<td>Very very bad <br>&bull; Invalid and unreliable measure <br>&bull; Lots of noise, and signal is pointing at the wrong direction</td>
<td>Problematic, but can live with <br>&bull; Valid, but unreliable measure <br>&bull; Lots of noise, harder to detect the signal; More likely to get false negative</td>
</tr>

<tr>
<td><strong>Random error = Low</strong></td>
<td>Problematic <br>&bull; Invalid, but reliable measure <br>&bull; Measure does not capture the concept of interests; Conclusion does not bear on the actual phenomenon of interest</td>
<td>Awesome! <br>&bull; Valid and reliable measure <br>&bull; Move along</td>
</tr>
</tbody>
</table>

<p>Ideally, we should try to minimize both types of measurement errors. Degree of random error can be empirically assessed (e.g. using Cronbach&rsquo;s alpha, see example <a href="https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/" target="_blank">here</a>), and can be reduced (e.g. using multiple indicators). Systematic error however, is harder to detect, harder to quantify, and harder to correct for.</p>

<h4 id="about-the-true-score-theory-t-x-epsilon-how-do-we-know-how-close-our-measured-value-x-is-close-to-the-true-score-t-if-we-cannot-truly-know-t">About the True Score Theory $T = X + \epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$?</h4>

<p>Unfortunately, we can never be 100% sure what the value of $T$ is. As mentioned above, while we can detect and correct for random errors, systematic errors cannot be corrected using statistical procedures. After we’ve done our best to minimize random error, it is up to the strength of our theory, clarity of conceptualization, and a small leap of faith to convince others (and ourselves), that our measures are indeed valid ones. This is also part of the reason why social science research can only establish a probabilistic relationship (confident within a certain range) and never a deterministic relationship. Embrace the uncertainty!</p>




<figure>

<img src="https://ih1.redbubble.net/image.184209277.1539/flat,1000x1000,075,f.jpg" width="400" />


</figure>

<h4 id="if-all-indicators-are-measured-with-some-degrees-of-random-errors-can-too-many-indicators-introduce-more-random-errors">If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?</h4>

<p>Although every single indicator would be measured with some random errors, if we combine the multiple indicators as an index, or take the average value, we should have lower random errors compared to using a single indicator.</p>

<h2 id="measurement-reliability-and-validity">Measurement Reliability and Validity</h2>

<h4 id="is-there-a-good-analogy-to-help-remembering-the-difference-between-validity-and-reliability">Is there a good analogy to help remembering the difference between validity and reliability?</h4>

<p>In class, I&rsquo;ve made the analogy comparing a valid indicator as a correct label (indicator) matching the content of a box (concept) that you wanted to buy but cannot see what is inside.</p>




<figure>

<img src="label_box.png" alt="Houston, we have a invalid indicator problem." />



<figcaption data-pre="Figure " data-post=":" >
  
  <p>
    Houston, we have a invalid indicator problem.
    
    
    
  </p> 
</figcaption>

</figure>

<p>I don&rsquo;t really have a good one for reliability, so let&rsquo;s stretch the same label-on-a-box analogy a bit further. Suppose we have a machine printing the label for the box, although the label correctly matches the box content (valid indicator), the machine sometimes misprints a letter or two, so not all labels look the same (unreliable). And if we have Machine A that produces 5% misprinted labels, and Machine B that produces 15% misprinted labels, then we can say that B is <em>less reliable</em> (produces less consistent outcomes).</p>

<h4 id="what-are-some-examples-of-face-validity">What are some examples of face validity?</h4>

<p>Whenever you see a indicator used to measure a particular concept, simply ask yourself: does the measure appear to capture the concept you care about? If yes, then the measure has high face validity; if not, then it has low face validity.</p>

<p>Say I want to measure whether a country&rsquo;s level of human rights protection, which of the following indicators has a higher face validity?</p>

<ul>
<li>Gini coefficient</li>
<li>Number of political imprisonment</li>
</ul>

<p>You probably have an answer in your mind. Let&rsquo;s try another one: now I want to measure a country&rsquo;s income inequality, which indicator has a higher face validity?</p>

<ul>
<li>Gini coefficient</li>
<li>Number of political imprisonment</li>
</ul>

<p>Again, you have an answer, and you are probably right.</p>

<p>A few things I&rsquo;d like to highlight from this example:</p>

<ul>
<li>Assessing face validity relies on domain knowledge. We need to first know what &ldquo;human rights protection&rdquo; means, only then we can see that more political imprisonment is an indicator for low levels of human rights protection.</li>
<li>Assessing face validity is largely based on judgment based on domain knowledge, rather than empirical demonstration.</li>
<li>Indicator validity is always assessed <em>relative</em> to the concept we are trying to capture, rather than something inherent to the indicator itself. Gini coefficient is a valid indicator for income inequality, but not human rights protection.</li>
</ul>

<h4 id="when-do-we-test-for-construct-vs-face-validity">When do we test for construct vs face validity?</h4>

<p>Ideally both, and more if possible. Since having invalid measures are really bad news, assessing the validity of a measure in multiple ways would increase the confidence</p>

<p>Face validity is rarely explicitly tested for &ndash; we already <em>implicitly</em> test for face validity when we are making the choice of which indicators to use to measure the concept. Although having face validity is important, high face validity alone is a rather weak evidence.</p>

<p>Construct validity can be empirically assessed in two ways: <em>convergent validity</em> and <em>divergent validity</em>. See <a href="https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/" target="_blank">here</a> for an example.</p>

<p>Generally, if we are using the measures that have been used in published literature, we do not have to conduct separate validity test. The assumption is that they have already been previously validated (though we should still remain critical). If we are using new measures in our study, instead of established ones used in published literature, then it is recommended to first conduct a pilot study to test the measure&rsquo;s validity and reliability. Use the measures as part of the actual study only after we know it&rsquo;s valid and reliable.</p>

<h4 id="about-the-article-https-www-nytimes-com-2014-08-28-opinion-nicholas-kristof-is-everyone-a-little-bit-racist-html-we-read-on-using-iat-video-games-to-measure-implicit-racial-bias-how-is-the-reliability-of-the-measure-determined-if-the-same-respondent-takes-the-test-twice-and-gets-different-scores-but-in-the-same-direction-e-g-at-first-longer-then-shorter-time-is-the-measure-considered-reliable">About the <a href="https://www.nytimes.com/2014/08/28/opinion/nicholas-kristof-is-everyone-a-little-bit-racist.html" target="_blank">article</a> we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?</h4>

<p>For IAT, the actual computation of the score takes quite a few steps, but to simplify it a bit, it is the <em>reaction time differential</em> that is used as a measure of implicit racial bias (see the test procedure <a href="https://en.wikipedia.org/wiki/Implicit-association_test#Procedure" target="_blank">here</a>). So in this case, the test can be considered as reliable if respondent has the same directional preference (e.g. consistently faster at White-Pleasant association, than Black-Pleasant association) when taking the test multiple times.</p>

<p>For other tests however, it could be the case that time difference itself, rather than directional difference is used as the measure.</p>

<p>In general, <a href="https://en.wikipedia.org/wiki/Repeatability" target="_blank">test-retest reliability</a> is measured as degree of correlation between the different test scores, rather than absolute difference. In psychology, rule of thumb is that test-retest reliability &gt; 0.7 is an acceptable level, though this is no more than a convention used by researchers. IAT has a test-retest reliability of about <a href="https://en.wikipedia.org/wiki/Implicit-association_test#Reliability" target="_blank">0.6</a>.</p>

<p>This <a href="https://fourbeers.fireside.fm/13" target="_blank">Podcast</a> has a pretty interesting discussion on the use and critique of IAT.</p>

<h2 id="levels-of-measurement">Levels of Measurement</h2>

<h4 id="can-you-elaborate-more-on-meaningful-vs-relative-arbitrary-zero-point-and-how-that-relates-to-interval-and-ratio-measures">Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures?</h4>

<p>A variable with meaningful zero point means that we can interpret the zero value as the <em>absence</em> of that variable. For example, income measured in dollars has a meaningful zero — we can interpret <code>income = 0</code> to mean an absence of income. So if someone reported zero on this measure, we know this person has no income.</p>

<p>On the other hand, if the variable has a relative, or arbitrary zero points, we cannot interpret the zero value on that variable as the <em>absence</em> of that variable. Say we have a set of 5 questions to measure people’s political knowledge. Every correct answer gets you 1 point, and every wrong answer gets you 0 point, which gives us a range of possible score from 0 to 5. If Ann gets <code>score = 0</code> on this scale, we cannot say that Ann has no political knowledge at all. The zero here is simply an arbitrary point to signal a very low level of political knowledge.</p>

<p>So how does this relate to interval vs ratio measures? Interval measures have relative/arbitrary zero points, and ratio measures have absolute/meaningful zero points. For the most part, the difference is only apparent (or we only need to pay attention to the difference) when we analyze and interpret the data.</p>

<p>For interval measures, since the zero point is arbitrary and lacks any meaningful interpretation, we cannot compare any differences in terms of proportion. It only make sense to compare the difference in magnitude. Going back to the political knowledge example, if Beth gets <code>score = 2</code> on the political scale, and Cathy gets <code>score = 4</code> on the same scale, we know that: 1) Cathy is more knowledgeable than Beth, and 2) the magnitude of difference is 2 more correct answers. However, since the zero point is arbitrary in this case, we <em>cannot</em> say Cathy is two times more knowledgeable than Beth. Or if we observe that Beth&rsquo;s score increased from 2 to 3 after attending a civics education workshop, we <em>cannot</em> cay that Beth&rsquo;s political knowledge increased by 50%.</p>

<p>Let&rsquo;s compare to a ratio scale, income, which has a meaningful zero point. If Abe reported <code>income = 20k</code>, and Ben reported <code>income = 40k</code>, we know that 1) Ben has higher income than Abe, 2) Ben&rsquo;s income is 20k higher than Abe, and 3) that Ben has an income twice as much as Abe.</p>

<!-- In fact, for most latent constructs / concepts, the indicators only have relative/arbitrary zero points, although we can construct a measure with a meaningful zero. For example, we can measure legislator ideology on left-right dimension using their voting record. Some legislators might be voting 100% left or 100% right. This indicator itself is a ratio measure — A who voted 80% left has twice as much liberal voting record as B who voted 40% left, and 0% liberal votes means someone has never voted for a liberal position on any bills (absence of liberal vote). Can we then interpret that A is twice as liberal as B? -->

<h4 id="can-a-measure-be-both-interval-and-ratio">Can a measure be both interval and ratio?</h4>

<p>The four levels of measurement are <em>mutually exclusive</em> categories. The flow chart below should help you to distinguish the four categories.</p>




<figure>

<img src="measure_levels.png" />


</figure>

<!--
```mermaid
graph TB
  A[Are the numbers just placeholders for categories <br> or do they have meaningful numerical values?] -- categories ---
  A1[Are the categories ordered<br> or unordered?]

  A1 -- unordered --- B1[Nominal]
  A1 -- ordered --- B2[Ordinal]

  A -- numerical --- A2[Is the zero value arbitrary<br> or meaningful?]

  A2 -- arbitrary --- C1[Interval]
  A2 -- meaningful --- C2[Ratio]
``` -->

<!-- ```mermaid
graph TB
  A[Are the researchers directly <br> manipulating the treatment?] -- yes --- A1
  A -- no --- A2[Is there an active change in the <br> treatment due to 'nature'?]

  A1[Is the treatment <br> randomly assigned?] -- yes --- B1[Randomized experiment]
  A1 -- no --- B2[Quasi-experiment]

  A2 -- yes --- C1[Natural experiment]
  A2 -- no --- C2[Observational design: Do we have single <br> or repeated measures for the same unit?]

  C2 -- single --- D1[Cross-sectional]
  C2 -- repeated --- D2[Longitudinal: <br> Panel / Time-series]
``` -->

<!-- ```mermaid
graph TB
  A[Is the treatment assignment <br> random or as-if random?] -- yes --- A1
  A -- no --- A2[Are the researchers directly <br> manipulating the treatment?]

  A1[Are the researchers directly <br> manipulating the treatment?] -- yes --- B1[Randomized experiment]
  A1 -- no --- B2[Natural experiment]

  A2 -- yes --- C1[Quasi-experiment]
  A2 -- no --- C2[Observational design: Do we have single <br> or repeated measures for the same unit?]

  C2 -- single --- D1[Cross-sectional]
  C2 -- repeated --- D2[Longitudinal: <br> Panel / Time-series]
``` -->

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/ps0700/tags/measurement/">Measurement</a>
  
</div>



    






<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/ps0700/">Research Methods in Political Science</a></h5>
    <h6 class="card-subtitle">Supplemental course materials for Spring 2019.</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="mailto:fanghui.z13@gmail.com" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>




    
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/" rel="next">Example: Testing for measurement validity and reliability</a>
  </div>
  
  
</div>

    </div>
    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "fanghuizhao" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; Fanghui Zhao 2019 <i class="fas fa-tree" style="color: #40a990;"></i> &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/ps0700/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/markdown.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/stata.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//fanghuizhao.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/ps0700/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    <script src="/ps0700/js/academic.min.70f0041f5a24c6a675ac218c98d7ef71.js"></script>

    

  </body>
</html>

