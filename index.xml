<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Political Science Research Methods on Political Science Research Methods</title>
    <link>https://fanghuiz.github.io/ps0700/</link>
    <description>Recent content in Political Science Research Methods on Political Science Research Methods</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Fanghui Zhao 2019 &lt;i class=&#34;fas fa-tree&#34; style=&#34;color: #40a990;&#34;&gt;&lt;/i&gt;</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/ps0700/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Q&amp;A Week 3: Experiments and Ethics</title>
      <link>https://fanghuiz.github.io/ps0700/post/2019-01-25-experiment/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>https://fanghuiz.github.io/ps0700/post/2019-01-25-experiment/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#about-informed-consent&#34;&gt;About Informed Consent&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#is-informed-consent-always-necessary-when-considering-the-ethics-of-social-science-experiments-for-some-experiments-obtaining-informed-consent-could-affect-the-results-if-people-are-aware-of-what-the-researchers-are-trying-to-do-exactly-how-much-are-the-researchers-required-to-inform-the-participants-about-the-experiment&#34;&gt;Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#does-knowing-you-are-part-of-an-experiment-affect-how-they-respond-is-there-a-way-to-minimize-the-effects-of-this-on-the-outcome&#34;&gt;Does knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#about-the-montana-gotv-experiment&#34;&gt;About the Montana GOTV Experiment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-montana-experiment-https-thewpsa-wordpress-com-2014-10-25-messing-with-montana-get-out-the-vote-experiment-raises-ethics-questions-misled-the-people-by-using-official-seal-how-did-they-get-the-project-approved-in-the-first-place&#34;&gt;The &lt;a href=&#34;https://thewpsa.wordpress.com/2014/10/25/messing-with-montana-get-out-the-vote-experiment-raises-ethics-questions/&#34; target=&#34;_blank&#34;&gt;Montana experiment&lt;/a&gt; misled the people by using official seal. How did they get the project approved in the first place?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-did-the-montana-experiment-affected-people-s-decision-i-don-t-see-a-discussion-on-how-it-actually-influenced-the-turnout-or-election-outcome&#34;&gt;How did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#about-experiments-on-development-programs&#34;&gt;About Experiments on Development Programs&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#are-there-examples-of-ethical-and-effective-anti-poverty-experiments&#34;&gt;Are there examples of ethical and effective anti-poverty experiments?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#not-a-question-just-an-interesting-observation-in-the-us-during-the-1960s-70s-there-was-a-similar-program-to-universal-basic-income-it-was-ended-after-there-was-an-increase-in-divorce-rate&#34;&gt;Not a question, just an interesting observation &amp;ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;about-informed-consent&#34;&gt;About Informed Consent&lt;/h2&gt;

&lt;h4 id=&#34;is-informed-consent-always-necessary-when-considering-the-ethics-of-social-science-experiments-for-some-experiments-obtaining-informed-consent-could-affect-the-results-if-people-are-aware-of-what-the-researchers-are-trying-to-do-exactly-how-much-are-the-researchers-required-to-inform-the-participants-about-the-experiment&#34;&gt;Is informed consent always necessary when considering the ethics of social science experiments? For some experiments, obtaining informed consent could affect the results (if people are aware of what the researchers are trying to do). Exactly how much are the researchers required to inform the participants about the experiment?&lt;/h4&gt;

&lt;p&gt;Yes, informed consent is an essential element of research ethics.&lt;/p&gt;

&lt;p&gt;Generally speaking, we have to inform the participants the purpose of our research (e.g. &lt;em&gt;“This is a study about attitudes towards political candidates&lt;/em&gt;), though we do not have to tell them the exact hypothesis of the study.&lt;/p&gt;

&lt;p&gt;It is also important that the informed consent form has to let the participants know if there is any potential benefits or harms by taking part in the study, any compensations or incentives, confidentiality or privacy of the data, their rights to decline and to withdraw, so they can make an informed decision about participating. In most cases, political science experiments only involve &amp;ldquo;minimal risks&amp;rdquo;, i.e. about the same probability and magnitude of harm we would experience in daily life.&lt;/p&gt;

&lt;p&gt;For more details on the important elements to include when obtaining informed consent, see &lt;a href=&#34;https://www.irb.pitt.edu/content/chapter-13-informed-consent-and-documentation&#34; target=&#34;_blank&#34;&gt;this guide&lt;/a&gt; from Pitt IRB, or American Psychological Association (APA) &lt;a href=&#34;https://www.apa.org/ethics/code/&#34; target=&#34;_blank&#34;&gt;ethics code&lt;/a&gt; (Section 8.02). It is possible to request waivers with adequate justification (see &lt;a href=&#34;http://www.irb.pitt.edu/sites/default/files/waivers%20presentation%204.10.18.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for an overview of the requirements).&lt;/p&gt;

&lt;h4 id=&#34;does-knowing-you-are-part-of-an-experiment-affect-how-they-respond-is-there-a-way-to-minimize-the-effects-of-this-on-the-outcome&#34;&gt;Does knowing you are part of an experiment affect how they respond? Is there a way to minimize the effects of this on the outcome?&lt;/h4&gt;

&lt;p&gt;Quite likely! One possibility is &lt;a href=&#34;https://en.wikipedia.org/wiki/Hawthorne_effect&#34; target=&#34;_blank&#34;&gt;Hawthorne effect&lt;/a&gt;: simply being part of the experiment and knowing that you are being observed might change your behavior or how you respond, compare to everyday life scenario.&lt;/p&gt;

&lt;p&gt;A more general phenomenon (some argue subsumes the Hawthorne effect) is called &lt;a href=&#34;https://en.wikipedia.org/wiki/Demand_characteristics&#34; target=&#34;_blank&#34;&gt;demand characteristics&lt;/a&gt; (also see textbook p.178), referring to how participants&amp;rsquo; interpretation of the experiment&amp;rsquo;s purpose could potentially change their behaviors (e.g. behave in ways &lt;em&gt;conforming&lt;/em&gt; to what they &lt;em&gt;think&lt;/em&gt; the researchers want to observe, or they might behave in ways &lt;em&gt;contradicting&lt;/em&gt; to what they perceived as the researchers&amp;rsquo; hypothesis).&lt;/p&gt;

&lt;p&gt;It is worth noting however, that not all experiments are equally affected by this potential problem. We might expect that experiments looking at behaviors that are more susceptible to social desirability bias are more vulnerable to bias introduced by demand characteristics, than those looking at more benign phenomenons.&lt;/p&gt;

&lt;p&gt;While it is difficult to eliminate this effect completely in most experiments, some strategies exist. For example, researchers can devise a design that uses covert or unobtrusive treatments, so the participants are unaware that they are part of an experiment (e.g. &lt;a href=&#34;https://scholar.harvard.edu/files/renos/files/enostrains.pdf&#34; target=&#34;_blank&#34;&gt;Enos 2014&lt;/a&gt;, &lt;a href=&#34;https://www.pnas.org/content/114/4/663.full&#34; target=&#34;_blank&#34;&gt;Sands 2017&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Deception is another common, though &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/10508420701712990&#34; target=&#34;_blank&#34;&gt;deeply&lt;/a&gt; &lt;a href=&#34;https://thepsychologist.bps.org.uk/volume-24/edition-8/deception-psychological-research-necessary-evil&#34; target=&#34;_blank&#34;&gt;controversial&lt;/a&gt; strategy. For example &lt;a href=&#34;https://en.wikipedia.org/wiki/Audit_study&#34; target=&#34;_blank&#34;&gt;audit experiments&lt;/a&gt; often rely on deception to examine socially undesirable behaviors such as discrimination (e.g. &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2011.00515.x&#34; target=&#34;_blank&#34;&gt;Butler and Broockman 2011&lt;/a&gt;), norms or rules violation (e.g. &lt;a href=&#34;http://www.michael-findley.com/uploads/2/0/4/5/20455799/findley_et_al.causes_of_non-compliance.ajps.10sep14.earlyview.pdf&#34; target=&#34;_blank&#34;&gt;Findley, Neilson and Sharman 2014&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Of course, the use of deception always has to be justified in the ethics review process. See this &lt;a href=&#34;http://scholar.harvard.edu/files/dtingley/files/spring2012.pdf&#34; target=&#34;_blank&#34;&gt;newsletter&lt;/a&gt; (p.13-19) for further discussion on the ethics of using deception in field experiment involving public officials as subjects.&lt;/p&gt;

&lt;h2 id=&#34;about-the-montana-gotv-experiment&#34;&gt;About the Montana GOTV Experiment&lt;/h2&gt;

&lt;h4 id=&#34;the-montana-experiment-https-thewpsa-wordpress-com-2014-10-25-messing-with-montana-get-out-the-vote-experiment-raises-ethics-questions-misled-the-people-by-using-official-seal-how-did-they-get-the-project-approved-in-the-first-place&#34;&gt;The &lt;a href=&#34;https://thewpsa.wordpress.com/2014/10/25/messing-with-montana-get-out-the-vote-experiment-raises-ethics-questions/&#34; target=&#34;_blank&#34;&gt;Montana experiment&lt;/a&gt; misled the people by using official seal. How did they get the project approved in the first place?&lt;/h4&gt;

&lt;p&gt;Only the people involved in the process would ever know! If I were to hazard a guess (take it with many many grains of salt), it is possible that the review process did not see the mailer as being intentionally misleading. Among the commotion in the follow-up to this controversy, one detail about the mailer did not get much attention — there was in fact a disclaimer line disclosing that the mailer is part of a academic study (below the boxes indicating candidate ideology).&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;montana_mailer.png&#34; alt=&#34;Mailer from the experiment. Squint a little to see the disclaimer. Retrieved from&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Mailer from the experiment. Squint a little to see the disclaimer. Retrieved from
    &lt;a href=&#34;https://web.archive.org/web/20170701171915/http://politicalpractices.mt.gov/content/2recentdecisions/McCullochvStanfordandDartmouthComplaint&#34;&gt; 
    Internet Archive
    &lt;/a&gt; 
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Maybe it’s too much of a fine print, but it’s there. So you could make the argument that they are not actively trying to deceive the recipient about who is sending the mailer out, and this might be part of reason why the proposal was approved. Again, I have to emphasize that this is all speculations on my part.&lt;/p&gt;

&lt;h4 id=&#34;how-did-the-montana-experiment-affected-people-s-decision-i-don-t-see-a-discussion-on-how-it-actually-influenced-the-turnout-or-election-outcome&#34;&gt;How did the Montana experiment affected people’s decision? I don’t see a discussion on how it actually influenced the turnout or election outcome.&lt;/h4&gt;

&lt;p&gt;We might never know! After the whole debacle, the study is unpublishable. Partly due to the ethical issue, partly because the data is likely unusable, given the spillover/contamination effect caused by the news coverage. After the news outlets reported about the story, those in the treatment group who have received the mailer would have known about where this mailer comes from and why they are receiving it (treatment is contaminated by extraneous factors that the researchers did not intend to provide), and those in the control group would also have known about the information in the mailer despite not receiving one (treatment spillover).&lt;/p&gt;

&lt;h2 id=&#34;about-experiments-on-development-programs&#34;&gt;About Experiments on Development Programs&lt;/h2&gt;

&lt;h4 id=&#34;are-there-examples-of-ethical-and-effective-anti-poverty-experiments&#34;&gt;Are there examples of ethical and effective anti-poverty experiments?&lt;/h4&gt;

&lt;p&gt;There are many examples of using randomized experiments to evaluate the impacts of anti-poverty programs. Some good places to look for them: &lt;a href=&#34;https://www.povertyactionlab.org/&#34; target=&#34;_blank&#34;&gt;Poverty Action Lab (J-PAL)&lt;/a&gt; (research center at MIT), &lt;a href=&#34;https://www.givewell.org/&#34; target=&#34;_blank&#34;&gt;GiveWell&lt;/a&gt; (nonprofit focused on effective charities).&lt;/p&gt;

&lt;h4 id=&#34;not-a-question-just-an-interesting-observation-in-the-us-during-the-1960s-70s-there-was-a-similar-program-to-universal-basic-income-it-was-ended-after-there-was-an-increase-in-divorce-rate&#34;&gt;Not a question, just an interesting observation &amp;ndash; in the US during the 1960s-70s, there was a similar program to Universal Basic Income. It was ended after there was an increase in divorce rate.&lt;/h4&gt;

&lt;p&gt;Hmm, this is really interesting to know. So if the experiment shows that UBI improves some aspects of life quality (e.g. household income, children&amp;rsquo;s education), but also has other &amp;ldquo;side-effects&amp;rdquo; such as increases divorce rate, from the policy-makers&amp;rsquo; position, what should they make of this? What kind of &amp;ldquo;side-effects&amp;rdquo;, or how much, would be considered as a &amp;ldquo;reasonable&amp;rdquo; level of trade-off? Back to what we discussed in the beginning of the course, empirical evidence does not always lead to a neat solution to normative questions.&lt;/p&gt;

&lt;!-- ## About Ethics and Experiments in General

#### If the studies were called out for being unethical, do researchers try fix the ethical concerns then re-do the experiment?


#### Facebook is a private corporation, people choose to use its service and agreed to the terms of services. Why do they get so much bad press for it? --&gt;
</description>
    </item>
    
    <item>
      <title>Example: Testing for measurement validity and reliability</title>
      <link>https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/</guid>
      <description>

&lt;h2 id=&#34;example-racial-resentment-scale&#34;&gt;Example: Racial Resentment Scale&lt;/h2&gt;

&lt;p&gt;Racial resentment scale is commonly used to measure &lt;a href=&#34;https://en.wikipedia.org/wiki/Symbolic_racism&#34; target=&#34;_blank&#34;&gt;symbolic racism&lt;/a&gt;. The scale contains four items, for each question, respondents indicate whether they agree or disagree with the statement on a five-point scale. The question wording and the respective variable number as appeared in American National Election Studies (ANES) 2016 are given below:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;V162211&lt;/code&gt;: &amp;lsquo;Irish, Italians, Jewish and many other minorities overcame prejudice and worked their way up. Blacks should do the same without any special favors.&amp;rsquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V162212&lt;/code&gt;: &amp;lsquo;Generations of slavery and discrimination have created conditions that make it difficult for blacks to work their way out of the lower class.&amp;rsquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V162213&lt;/code&gt;: &amp;lsquo;Over the past few years, blacks have gotten less than they deserve.&amp;rsquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V162214&lt;/code&gt;: &amp;lsquo;It’s really a matter of some people not trying hard enough, if blacks would only try harder they could be just as well off as whites.&amp;rsquo;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The assumption is that agreeing with statement 1 and 4 (or disagreeing with statement 2 and 3) are indications of resentment towards African Americans.&lt;/p&gt;

&lt;h2 id=&#34;validity-test&#34;&gt;Validity Test&lt;/h2&gt;

&lt;h3 id=&#34;construct-validity&#34;&gt;Construct validity&lt;/h3&gt;

&lt;p&gt;To test for construct validity, we need to demonstrate that the indicator predicts what it is supposed to predict.&lt;/p&gt;

&lt;p&gt;One aspect of construct validity is &lt;strong&gt;convergent validity&lt;/strong&gt;: if theoretically we expect X and Y to be positively related, do we see a positive correlation between the indicator for X and Y?&lt;/p&gt;

&lt;p&gt;In this case, theoretically we might expect that feelings of resentment towards African Americans would correlate with negative affective attitudes towards the group.&lt;/p&gt;

&lt;p&gt;For illustration purposes, let&amp;rsquo;s just use a single statement from the resentment scale, statement 2 (&lt;code&gt;V162212&lt;/code&gt;) about the effects of slavery and see if people&amp;rsquo;s answer to this questions correlates with their feeling thermometer score towards Blacks (&lt;code&gt;V162312&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;. twoway (scatter V162312 V162212) (lfit V162312 V162212)

&lt;/code&gt;&lt;/pre&gt;

<<<<<<< HEAD
&lt;!-- 


&lt;figure&gt;

&lt;img src=&#34;https://fanghuiz.github.io/ps0700/img/stata/validity_1.svg&#34; /&gt;


&lt;/figure&gt; --&gt;




&lt;figure&gt;

&lt;img src=&#34;https://fanghuiz.github.io/ps0700/img/stata/validity_1.svg&#34; /&gt;


&lt;/figure&gt;
=======
&lt;p&gt;&lt;img src=&#34;https://fanghuiz.github.io/ps0700/img/stata/validity_1.svg&#34; alt=&#34;validity_1&#34; /&gt;&lt;/p&gt;
>>>>>>> parent of 15eb630... Deleted docs folder

&lt;p&gt;We see that higher disagreement with the statement (&lt;code&gt;1 = Strongly Agree&lt;/code&gt;, &lt;code&gt;5 = Strongly Disagree&lt;/code&gt;) correlates with lower scores on the feeling thermometer (higher value means warmer feeling towards the group, lower value means colder feeling). Resentment towards African Americans (as indicated by denying the effects of slavery on their current day hardship) indeed predicts a more negative attitudes towards them (as indicated by expressing less warm feelings).&lt;/p&gt;

&lt;p&gt;Another way to demonstrate construct validity is to show &lt;strong&gt;divergent/discriminant validity&lt;/strong&gt;: if theoretically we &lt;em&gt;do not&lt;/em&gt; expect X and Y to be related, do we then see a low or weak correlation between them?&lt;/p&gt;

&lt;p&gt;For instance, perhaps we do not expect feelings of racial resentment to be correlated with feelings towards the Supreme Court (&lt;code&gt;V162102&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;. graph twoway (scatter V162102 V162212) (lfit V162102 V162212)

&lt;/code&gt;&lt;/pre&gt;

<<<<<<< HEAD
&lt;!-- 


&lt;figure&gt;

&lt;img src=&#34;https://fanghuiz.github.io/ps0700/img/stata/validity_2.svg&#34; /&gt;


&lt;/figure&gt; --&gt;




&lt;figure&gt;

&lt;img src=&#34;https://fanghuiz.github.io/ps0700/img/stata/validity_2.svg&#34; /&gt;


&lt;/figure&gt;
=======
&lt;p&gt;&lt;img src=&#34;https://fanghuiz.github.io/ps0700/img/stata/validity_2.svg&#34; alt=&#34;validity_2&#34; /&gt;&lt;/p&gt;
>>>>>>> parent of 15eb630... Deleted docs folder

&lt;p&gt;We see that there is no discernible correlation between responses to statement 2 and feelings towards the Supreme Court.&lt;/p&gt;

&lt;h2 id=&#34;reliability-test&#34;&gt;Reliability Test&lt;/h2&gt;

&lt;p&gt;One common way to quantify the reliability of a multiple indicator scale is to calculate the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cronbach%27s_alpha&#34; target=&#34;_blank&#34;&gt;Cronbach&amp;rsquo;s alpha $\alpha$&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This can be done in Stata using a simple command &lt;code&gt;alpha&lt;/code&gt;, followed by the list of variables used in the scale.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;. alpha V162211 V162212 V162213 V162214

Test scale = mean(unstandardized items)
Reversed items:  V162211 V162214

Average interitem covariance:     1.090995
Number of items in the scale:            4
Scale reliability coefficient:      0.8451

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the output, we can see a &amp;ldquo;Scale reliability coefficient&amp;rdquo;, which is ~0.8 in this case. A general rule of thumb is that &amp;gt; 0.8 indicates rather high reliability, and anything below 0.7 is a sign of unreliable scale. So in this case, the four-item Racial Resentment Scale has rather high reliability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Q&amp;A Week 2: Measurement</title>
      <link>https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/</link>
      <pubDate>Fri, 18 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>https://fanghuiz.github.io/ps0700/post/2019-01-18-measurement/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-measurement-errors&#34;&gt;Types of measurement Errors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-differentiate-between-systematic-vs-random-error-do-you-have-any-examples-of-systematic-errors-in-measurement&#34;&gt;How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#which-error-systematic-vs-random-is-worse-which-one-should-we-try-to-avoid-more&#34;&gt;Which error (systematic vs random) is worse? Which one should we try to avoid more?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#about-the-true-score-theory-t-x-epsilon-how-do-we-know-how-close-our-measured-value-x-is-close-to-the-true-score-t-if-we-cannot-truly-know-t&#34;&gt;About the True Score Theory $T = X + \epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#if-all-indicators-are-measured-with-some-degrees-of-random-errors-can-too-many-indicators-introduce-more-random-errors&#34;&gt;If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measurement-reliability-and-validity&#34;&gt;Measurement Reliability and Validity&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#is-there-a-good-analogy-to-help-remembering-the-difference-between-validity-and-reliability&#34;&gt;Is there a good analogy to help remembering the difference between validity and reliability?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-are-some-examples-of-face-validity&#34;&gt;What are some examples of face validity?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-do-we-test-for-construct-vs-face-validity&#34;&gt;When do we test for construct vs face validity?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#about-the-article-https-www-nytimes-com-2014-08-28-opinion-nicholas-kristof-is-everyone-a-little-bit-racist-html-we-read-on-using-iat-video-games-to-measure-implicit-racial-bias-how-is-the-reliability-of-the-measure-determined-if-the-same-respondent-takes-the-test-twice-and-gets-different-scores-but-in-the-same-direction-e-g-at-first-longer-then-shorter-time-is-the-measure-considered-reliable&#34;&gt;About the &lt;a href=&#34;https://www.nytimes.com/2014/08/28/opinion/nicholas-kristof-is-everyone-a-little-bit-racist.html&#34; target=&#34;_blank&#34;&gt;article&lt;/a&gt; we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#levels-of-measurement&#34;&gt;Levels of Measurement&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#can-you-elaborate-more-on-meaningful-vs-relative-arbitrary-zero-point-and-how-that-relates-to-interval-and-ratio-measures&#34;&gt;Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#can-a-measure-be-both-interval-and-ratio&#34;&gt;Can a measure be both interval and ratio?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;types-of-measurement-errors&#34;&gt;Types of measurement Errors&lt;/h2&gt;

&lt;h4 id=&#34;how-to-differentiate-between-systematic-vs-random-error-do-you-have-any-examples-of-systematic-errors-in-measurement&#34;&gt;How to differentiate between systematic vs random error? Do you have any examples of systematic errors in measurement?&lt;/h4&gt;

&lt;p&gt;Systematic errors affects all units in the sample in the same direction (all measured values are consistently more positive or more negative than true value). For example, self-reported measure of turnout is likely to have positive systematic error &amp;ndash; (most) people tend to over-report, saying that they have voted even though they have not.&lt;/p&gt;

&lt;p&gt;Random errors do not affect all units in the sample in a consistent way &amp;ndash; some units will be more positive than true value, some units will be more negative than true value. Let&amp;rsquo;s use the self-reported turnout as an example again. Perhaps people&amp;rsquo;s transient feelings about the current election affect whether they are likely to say they have voted or not &amp;ndash; those who happened to read a positive news story about the election are more likely to over-report having voted, and others who happened to read a negative news story are more likely to under-report.&lt;/p&gt;

&lt;p&gt;Very often both types of errors could be present, so we need to think carefully about the sources of potential errors. For example, crime statistics can be very noisy, with a lot of random errors introduced at various stages of collecting such data. Furthermore, statistics on certain &lt;em&gt;types&lt;/em&gt; of crimes might additionally have systematic errors: for example, domestic abuse might be systematically biased downwards if victims under-report due to fear of retaliation.&lt;/p&gt;

&lt;!-- As the sample size increases, we are likely to see that there are as many positive errors as negative ones, and all the random errors would sum to 0. This means that random errors add *variability/noise* to the data, but will not affect the average value at the *group* level. --&gt;

&lt;h4 id=&#34;which-error-systematic-vs-random-is-worse-which-one-should-we-try-to-avoid-more&#34;&gt;Which error (systematic vs random) is worse? Which one should we try to avoid more?&lt;/h4&gt;

&lt;p&gt;Both types of errors are bad news! But they affect our analysis in different ways.&lt;/p&gt;

&lt;p&gt;High random errors will add more &lt;em&gt;noise/variability&lt;/em&gt; to our data, which will make it harder to detect the presence of a significant correlation between X and Y. In another word, noisy measures are bad because it increases the likelihood of &lt;em&gt;false negative&lt;/em&gt; &amp;ndash; we are likely to mistakenly infer there is no relationship between X and Y, when in fact there is.&lt;/p&gt;

&lt;p&gt;For systematic errors, recall that indicators with high systematic errors are &lt;em&gt;invalid&lt;/em&gt;, i.e. they are not capturing the concept of interest accurately. In such case, an invalid indicator will &lt;em&gt;never&lt;/em&gt; lead us to the right conclusion (think of a road sign that points to the wrong direction), even if the indicator is measured with zero random error.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;https://vignette.wikia.nocookie.net/thebige/images/d/da/Funny_road_signs_5.jpg/revision/latest/scale-to-width-down/180?cb=20130807005626&#34; alt=&#34;One of them has got to be invalid..&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    One of them has got to be invalid..
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In terms of which type of error is &lt;em&gt;worse&lt;/em&gt;, one way I think about this is that invalid indicator is more like a fatal disease, and unreliable indicator is more like a non-fatal but chronic disease that requires lots of care. So if a study is using invalid indicators, we cannot draw any meaningful inferences about the phenomenon we are investigating (the study is &amp;ldquo;dead&amp;rdquo;), while unreliable indicators make it harder to detect a &lt;em&gt;true positive&lt;/em&gt; (increases uncertainty, but does not spell doom).&lt;/p&gt;

&lt;p&gt;The textbook also has a good discussion on the different problems associated with measurement reliability and validity in political science (p.143-145).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Systematic error = High&lt;/th&gt;
&lt;th&gt;Systematic error = Low&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Random error = High&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Very very bad &lt;br&gt;&amp;bull; Invalid and unreliable measure &lt;br&gt;&amp;bull; Lots of noise, and signal is pointing at the wrong direction&lt;/td&gt;
&lt;td&gt;Problematic, but can live with &lt;br&gt;&amp;bull; Valid, but unreliable measure &lt;br&gt;&amp;bull; Lots of noise, harder to detect the signal; More likely to get false negative&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Random error = Low&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Problematic &lt;br&gt;&amp;bull; Invalid, but reliable measure &lt;br&gt;&amp;bull; Measure does not capture the concept of interests; Conclusion does not bear on the actual phenomenon of interest&lt;/td&gt;
&lt;td&gt;Awesome! &lt;br&gt;&amp;bull; Valid and reliable measure &lt;br&gt;&amp;bull; Move along&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Ideally, we should try to minimize both types of measurement errors. Degree of random error can be empirically assessed (e.g. using Cronbach&amp;rsquo;s alpha, see example &lt;a href=&#34;https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), and can be reduced (e.g. using multiple indicators). Systematic error however, is harder to detect, harder to quantify, and harder to correct for.&lt;/p&gt;

&lt;h4 id=&#34;about-the-true-score-theory-t-x-epsilon-how-do-we-know-how-close-our-measured-value-x-is-close-to-the-true-score-t-if-we-cannot-truly-know-t&#34;&gt;About the True Score Theory $T = X + \epsilon$, how do we know how close our measured value $X$ is close to the true score $T$, if we cannot truly know $T$?&lt;/h4&gt;

&lt;p&gt;Unfortunately, we can never be 100% sure what the value of $T$ is. As mentioned above, while we can detect and correct for random errors, systematic errors cannot be corrected using statistical procedures. After we’ve done our best to minimize random error, it is up to the strength of our theory, clarity of conceptualization, and a small leap of faith to convince others (and ourselves), that our measures are indeed valid ones. This is also part of the reason why social science research can only establish a probabilistic relationship (confident within a certain range) and never a deterministic relationship. Embrace the uncertainty!&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;https://ih1.redbubble.net/image.184209277.1539/flat,1000x1000,075,f.jpg&#34; width=&#34;400&#34; /&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;if-all-indicators-are-measured-with-some-degrees-of-random-errors-can-too-many-indicators-introduce-more-random-errors&#34;&gt;If all indicators are measured with some degrees of random errors, can too many indicators introduce more random errors?&lt;/h4&gt;

&lt;p&gt;Although every single indicator would be measured with some random errors, if we combine the multiple indicators as an index, or take the average value, we should have lower random errors compared to using a single indicator.&lt;/p&gt;

&lt;h2 id=&#34;measurement-reliability-and-validity&#34;&gt;Measurement Reliability and Validity&lt;/h2&gt;

&lt;h4 id=&#34;is-there-a-good-analogy-to-help-remembering-the-difference-between-validity-and-reliability&#34;&gt;Is there a good analogy to help remembering the difference between validity and reliability?&lt;/h4&gt;

&lt;p&gt;In class, I&amp;rsquo;ve made the analogy comparing a valid indicator as a correct label (indicator) matching the content of a box (concept) that you wanted to buy but cannot see what is inside.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;label_box.png&#34; alt=&#34;Houston, we have a invalid indicator problem.&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Houston, we have a invalid indicator problem.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;I don&amp;rsquo;t really have a good one for reliability, so let&amp;rsquo;s stretch the same label-on-a-box analogy a bit further. Suppose we have a machine printing the label for the box, although the label correctly matches the box content (valid indicator), the machine sometimes misprints a letter or two, so not all labels look the same (unreliable). And if we have Machine A that produces 5% misprinted labels, and Machine B that produces 15% misprinted labels, then we can say that B is &lt;em&gt;less reliable&lt;/em&gt; (produces less consistent outcomes).&lt;/p&gt;

&lt;h4 id=&#34;what-are-some-examples-of-face-validity&#34;&gt;What are some examples of face validity?&lt;/h4&gt;

&lt;p&gt;Whenever you see a indicator used to measure a particular concept, simply ask yourself: does the measure appear to capture the concept you care about? If yes, then the measure has high face validity; if not, then it has low face validity.&lt;/p&gt;

&lt;p&gt;Say I want to measure whether a country&amp;rsquo;s level of human rights protection, which of the following indicators has a higher face validity?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gini coefficient&lt;/li&gt;
&lt;li&gt;Number of political imprisonment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You probably have an answer in your mind. Let&amp;rsquo;s try another one: now I want to measure a country&amp;rsquo;s income inequality, which indicator has a higher face validity?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gini coefficient&lt;/li&gt;
&lt;li&gt;Number of political imprisonment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, you have an answer, and you are probably right.&lt;/p&gt;

&lt;p&gt;A few things I&amp;rsquo;d like to highlight from this example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Assessing face validity relies on domain knowledge. We need to first know what &amp;ldquo;human rights protection&amp;rdquo; means, only then we can see that more political imprisonment is an indicator for low levels of human rights protection.&lt;/li&gt;
&lt;li&gt;Assessing face validity is largely based on judgment based on domain knowledge, rather than empirical demonstration.&lt;/li&gt;
&lt;li&gt;Indicator validity is always assessed &lt;em&gt;relative&lt;/em&gt; to the concept we are trying to capture, rather than something inherent to the indicator itself. Gini coefficient is a valid indicator for income inequality, but not human rights protection.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;when-do-we-test-for-construct-vs-face-validity&#34;&gt;When do we test for construct vs face validity?&lt;/h4&gt;

&lt;p&gt;Ideally both, and more if possible. Since having invalid measures are really bad news, assessing the validity of a measure in multiple ways would increase the confidence&lt;/p&gt;

&lt;p&gt;Face validity is rarely explicitly tested for &amp;ndash; we already &lt;em&gt;implicitly&lt;/em&gt; test for face validity when we are making the choice of which indicators to use to measure the concept. Although having face validity is important, high face validity alone is a rather weak evidence.&lt;/p&gt;

&lt;p&gt;Construct validity can be empirically assessed in two ways: &lt;em&gt;convergent validity&lt;/em&gt; and &lt;em&gt;divergent validity&lt;/em&gt;. See &lt;a href=&#34;https://fanghuiz.github.io/ps0700/post/2019-01-19-example-measurement-stata/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; for an example.&lt;/p&gt;

&lt;p&gt;Generally, if we are using the measures that have been used in published literature, we do not have to conduct separate validity test. The assumption is that they have already been previously validated (though we should still remain critical). If we are using new measures in our study, instead of established ones used in published literature, then it is recommended to first conduct a pilot study to test the measure&amp;rsquo;s validity and reliability. Use the measures as part of the actual study only after we know it&amp;rsquo;s valid and reliable.&lt;/p&gt;

&lt;h4 id=&#34;about-the-article-https-www-nytimes-com-2014-08-28-opinion-nicholas-kristof-is-everyone-a-little-bit-racist-html-we-read-on-using-iat-video-games-to-measure-implicit-racial-bias-how-is-the-reliability-of-the-measure-determined-if-the-same-respondent-takes-the-test-twice-and-gets-different-scores-but-in-the-same-direction-e-g-at-first-longer-then-shorter-time-is-the-measure-considered-reliable&#34;&gt;About the &lt;a href=&#34;https://www.nytimes.com/2014/08/28/opinion/nicholas-kristof-is-everyone-a-little-bit-racist.html&#34; target=&#34;_blank&#34;&gt;article&lt;/a&gt; we read on using IAT/video games to measure implicit racial bias, how is the reliability of the measure determined? If the same respondent takes the test twice and gets different scores but in the same direction (e.g at first longer, then shorter time), is the measure considered reliable?&lt;/h4&gt;

&lt;p&gt;For IAT, the actual computation of the score takes quite a few steps, but to simplify it a bit, it is the &lt;em&gt;reaction time differential&lt;/em&gt; that is used as a measure of implicit racial bias (see the test procedure &lt;a href=&#34;https://en.wikipedia.org/wiki/Implicit-association_test#Procedure&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;). So in this case, the test can be considered as reliable if respondent has the same directional preference (e.g. consistently faster at White-Pleasant association, than Black-Pleasant association) when taking the test multiple times.&lt;/p&gt;

&lt;p&gt;For other tests however, it could be the case that time difference itself, rather than directional difference is used as the measure.&lt;/p&gt;

&lt;p&gt;In general, &lt;a href=&#34;https://en.wikipedia.org/wiki/Repeatability&#34; target=&#34;_blank&#34;&gt;test-retest reliability&lt;/a&gt; is measured as degree of correlation between the different test scores, rather than absolute difference. In psychology, rule of thumb is that test-retest reliability &amp;gt; 0.7 is an acceptable level, though this is no more than a convention used by researchers. IAT has a test-retest reliability of about &lt;a href=&#34;https://en.wikipedia.org/wiki/Implicit-association_test#Reliability&#34; target=&#34;_blank&#34;&gt;0.6&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;https://fourbeers.fireside.fm/13&#34; target=&#34;_blank&#34;&gt;Podcast&lt;/a&gt; has a pretty interesting discussion on the use and critique of IAT.&lt;/p&gt;

&lt;h2 id=&#34;levels-of-measurement&#34;&gt;Levels of Measurement&lt;/h2&gt;

&lt;h4 id=&#34;can-you-elaborate-more-on-meaningful-vs-relative-arbitrary-zero-point-and-how-that-relates-to-interval-and-ratio-measures&#34;&gt;Can you elaborate more on meaningful vs relative/arbitrary zero point, and how that relates to interval and ratio measures?&lt;/h4&gt;

&lt;p&gt;A variable with meaningful zero point means that we can interpret the zero value as the &lt;em&gt;absence&lt;/em&gt; of that variable. For example, income measured in dollars has a meaningful zero — we can interpret &lt;code&gt;income = 0&lt;/code&gt; to mean an absence of income. So if someone reported zero on this measure, we know this person has no income.&lt;/p&gt;

&lt;p&gt;On the other hand, if the variable has a relative, or arbitrary zero points, we cannot interpret the zero value on that variable as the &lt;em&gt;absence&lt;/em&gt; of that variable. Say we have a set of 5 questions to measure people’s political knowledge. Every correct answer gets you 1 point, and every wrong answer gets you 0 point, which gives us a range of possible score from 0 to 5. If Ann gets &lt;code&gt;score = 0&lt;/code&gt; on this scale, we cannot say that Ann has no political knowledge at all. The zero here is simply an arbitrary point to signal a very low level of political knowledge.&lt;/p&gt;

&lt;p&gt;So how does this relate to interval vs ratio measures? Interval measures have relative/arbitrary zero points, and ratio measures have absolute/meaningful zero points. For the most part, the difference is only apparent (or we only need to pay attention to the difference) when we analyze and interpret the data.&lt;/p&gt;

&lt;p&gt;For interval measures, since the zero point is arbitrary and lacks any meaningful interpretation, we cannot compare any differences in terms of proportion. It only make sense to compare the difference in magnitude. Going back to the political knowledge example, if Beth gets &lt;code&gt;score = 2&lt;/code&gt; on the political scale, and Cathy gets &lt;code&gt;score = 4&lt;/code&gt; on the same scale, we know that: 1) Cathy is more knowledgeable than Beth, and 2) the magnitude of difference is 2 more correct answers. However, since the zero point is arbitrary in this case, we &lt;em&gt;cannot&lt;/em&gt; say Cathy is two times more knowledgeable than Beth. Or if we observe that Beth&amp;rsquo;s score increased from 2 to 3 after attending a civics education workshop, we &lt;em&gt;cannot&lt;/em&gt; cay that Beth&amp;rsquo;s political knowledge increased by 50%.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s compare to a ratio scale, income, which has a meaningful zero point. If Abe reported &lt;code&gt;income = 20k&lt;/code&gt;, and Ben reported &lt;code&gt;income = 40k&lt;/code&gt;, we know that 1) Ben has higher income than Abe, 2) Ben&amp;rsquo;s income is 20k higher than Abe, and 3) that Ben has an income twice as much as Abe.&lt;/p&gt;

&lt;!-- In fact, for most latent constructs / concepts, the indicators only have relative/arbitrary zero points, although we can construct a measure with a meaningful zero. For example, we can measure legislator ideology on left-right dimension using their voting record. Some legislators might be voting 100% left or 100% right. This indicator itself is a ratio measure — A who voted 80% left has twice as much liberal voting record as B who voted 40% left, and 0% liberal votes means someone has never voted for a liberal position on any bills (absence of liberal vote). Can we then interpret that A is twice as liberal as B? --&gt;

&lt;h4 id=&#34;can-a-measure-be-both-interval-and-ratio&#34;&gt;Can a measure be both interval and ratio?&lt;/h4&gt;

&lt;p&gt;The four levels of measurement are &lt;em&gt;mutually exclusive&lt;/em&gt; categories. The flow chart below should help you to distinguish the four categories.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;measure_levels.png&#34; /&gt;


&lt;/figure&gt;

&lt;!--
```mermaid
graph TB
  A[Are the numbers just placeholders for categories &lt;br&gt; or do they have meaningful numerical values?] -- categories ---
  A1[Are the categories ordered&lt;br&gt; or unordered?]

  A1 -- unordered --- B1[Nominal]
  A1 -- ordered --- B2[Ordinal]

  A -- numerical --- A2[Is the zero value arbitrary&lt;br&gt; or meaningful?]

  A2 -- arbitrary --- C1[Interval]
  A2 -- meaningful --- C2[Ratio]
``` --&gt;

&lt;!-- ```mermaid
graph TB
  A[Are the researchers directly &lt;br&gt; manipulating the treatment?] -- yes --- A1
  A -- no --- A2[Is there an active change in the &lt;br&gt; treatment due to &#39;nature&#39;?]

  A1[Is the treatment &lt;br&gt; randomly assigned?] -- yes --- B1[Randomized experiment]
  A1 -- no --- B2[Quasi-experiment]

  A2 -- yes --- C1[Natural experiment]
  A2 -- no --- C2[Observational design: Do we have single &lt;br&gt; or repeated measures for the same unit?]

  C2 -- single --- D1[Cross-sectional]
  C2 -- repeated --- D2[Longitudinal: &lt;br&gt; Panel / Time-series]
``` --&gt;

&lt;!-- ```mermaid
graph TB
  A[&#34;Is the treatment assignment &lt;br&gt; random or &lt;i&gt; &#39;&#39;as-if&#39;&#39; &lt;/i&gt; random?&#34;] -- yes --- A1
  A -- no --- A2[Are the researchers directly &lt;br&gt; manipulating the treatment?]

  A1[Are the researchers directly &lt;br&gt; manipulating the treatment?] -- yes --- B1[Randomized experiment]
  A1 -- no --- B2[Natural experiment]

  A2 -- yes --- C1[Quasi-experiment]
  A2 -- no --- C2[Observational design: Do we have single &lt;br&gt; or repeated measures for the same unit?]

  C2 -- single --- D1[Cross-sectional]
  C2 -- repeated --- D2[Longitudinal: &lt;br&gt; Panel / Time-series]
``` --&gt;
</description>
    </item>
    
  </channel>
</rss>
